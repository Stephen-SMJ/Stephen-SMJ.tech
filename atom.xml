<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>小孙不够睡的博客</title>
  
  <subtitle>MyBlog</subtitle>
  <link href="https://stephen-smj.tech/atom.xml" rel="self"/>
  
  <link href="https://stephen-smj.tech/"/>
  <updated>2024-09-16T15:31:59.785Z</updated>
  <id>https://stephen-smj.tech/</id>
  
  <author>
    <name>Sun Maojun</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Understanding Decoding Methods in Large Language Models (LLMs) and Key Parameters</title>
    <link href="https://stephen-smj.tech/2024/08/05/Decode%20in%20LLM/"/>
    <id>https://stephen-smj.tech/2024/08/05/Decode%20in%20LLM/</id>
    <published>2024-08-05T12:07:34.000Z</published>
    <updated>2024-09-16T15:31:59.785Z</updated>
    
    <content type="html"><![CDATA[<p>When interacting with Large Language Models (LLMs) like GPT, BERT, or T5, the magic happens not only in the training but also in the <strong>decoding</strong> phase. Decoding is the process through which a language model generates output, and the choice of decoding method can greatly influence the quality of the generated text.</p><p>In this post, we’ll explore the common decoding techniques, explain key parameters such as <code>top-p</code> and <code>top-k</code>, and discuss how these models are trained to generate coherent and contextually accurate outputs.</p><h2 id="1-Introduction-to-Decoding-in-LLMs"><a href="#1-Introduction-to-Decoding-in-LLMs" class="headerlink" title="1. Introduction to Decoding in LLMs"></a>1. Introduction to Decoding in LLMs</h2><p>Decoding is the step where the model takes a sequence of probabilities generated by its neural network and converts it into human-readable text. This is not a straightforward task, as the model can produce many possible sequences. Choosing the right sequence involves controlling randomness and ensuring that the output is grammatically and contextually correct.</p><p>Here are the most common decoding techniques:</p><h3 id="1-1-Greedy-Decoding"><a href="#1-1-Greedy-Decoding" class="headerlink" title="1.1. Greedy Decoding"></a>1.1. Greedy Decoding</h3><p><strong>Greedy decoding</strong> selects the word with the highest probability at each step of generation. While simple, it can often result in repetitive or low-quality text because it doesn’t explore alternative words that might lead to better outcomes.</p><p><strong>Example</strong>:</p><ul><li>Input: “The cat”</li><li>Greedy Output: “The cat is on the mat.”</li></ul><h3 id="1-2-Beam-Search"><a href="#1-2-Beam-Search" class="headerlink" title="1.2. Beam Search"></a>1.2. Beam Search</h3><p><strong>Beam search</strong> is an improvement over greedy decoding. It keeps track of multiple possible sequences (called “beams”) at each step, choosing the best one after considering several alternatives. The size of the beam (<code>beam width</code>) determines how many sequences the model will consider.</p><p><strong>Example</strong>:</p><ul><li>Input: “The cat”</li><li>Beam Search Output: “The cat is sitting by the window.”</li></ul><p>Beam search provides more coherent and higher-quality outputs, but it can be computationally expensive.</p><h3 id="1-3-Top-k-Sampling"><a href="#1-3-Top-k-Sampling" class="headerlink" title="1.3. Top-k Sampling"></a>1.3. Top-k Sampling</h3><p><strong>Top-k sampling</strong> is a method where the model only considers the top <code>k</code> most probable words at each step, and it randomly selects one of them. This introduces more diversity and creativity in the output by not always choosing the most probable word.</p><ul><li><code>k</code>: The number of top words to sample from.</li></ul><p><strong>Example</strong>:</p><ul><li>Input: “The cat”</li><li>Top-k (with k=5) Output: “The cat jumped over the fence.”</li></ul><p>The higher the value of <code>k</code>, the more diverse and unexpected the text can become, but it might also lose coherence.</p><h3 id="1-4-Nucleus-Sampling-Top-p-Sampling"><a href="#1-4-Nucleus-Sampling-Top-p-Sampling" class="headerlink" title="1.4. Nucleus Sampling (Top-p Sampling)"></a>1.4. Nucleus Sampling (Top-p Sampling)</h3><p><strong>Top-p sampling</strong>, also known as <strong>nucleus sampling</strong>, improves on top-k by considering all words whose cumulative probability adds up to a threshold <code>p</code>. It adapts dynamically to the context of the generation, ensuring both diversity and coherence.</p><ul><li><code>p</code>: The cumulative probability threshold (e.g., p=0.9).</li></ul><p><strong>Example</strong>:</p><ul><li>Input: “The cat”</li><li>Top-p (with p=0.9) Output: “The cat darted across the room.”</li></ul><p>Top-p sampling adjusts the number of words sampled at each step, giving the model more flexibility compared to top-k, where the number of options is fixed.</p><h3 id="1-5-Temperature-Sampling"><a href="#1-5-Temperature-Sampling" class="headerlink" title="1.5. Temperature Sampling"></a>1.5. Temperature Sampling</h3><p><strong>Temperature</strong> controls the randomness of the output by scaling the probability distribution of the model. Higher temperatures make the model more likely to select less probable words, resulting in more creative but potentially less coherent text.</p><ul><li><code>temperature</code>: A value between 0 and 1 (default is 1). Lower values make the output more deterministic, while higher values increase randomness.</li></ul><p><strong>Example</strong>:</p><ul><li>Input: “The cat”</li><li>Temperature 0.5: “The cat is sleeping.”</li><li>Temperature 1.5: “The feline leaps across the horizon in pursuit of adventure.”</li></ul><p>A lower temperature makes the model more conservative, while a higher temperature makes it more creative.</p><hr><h2 id="2-Key-Parameters-in-Decoding"><a href="#2-Key-Parameters-in-Decoding" class="headerlink" title="2. Key Parameters in Decoding"></a>2. Key Parameters in Decoding</h2><p>Now that we understand the common decoding methods, let’s look at the parameters that control how these methods function and affect the quality of generated text.</p><h3 id="2-1-top-k"><a href="#2-1-top-k" class="headerlink" title="2.1. top-k"></a>2.1. <code>top-k</code></h3><p><strong><code>top-k</code></strong> is a parameter used in top-k sampling, determining the number of top possible words to sample from at each step. It balances between randomness and coherence.</p><ul><li><strong>Low <code>k</code> values</strong>: Result in more deterministic and repetitive text.</li><li><strong>High <code>k</code> values</strong>: Lead to more varied, creative text but may decrease coherence.</li></ul><p><strong>When to use?</strong>: Use top-k sampling when you want more diverse outputs, like in creative writing tasks or when generating longer texts.</p><h3 id="2-2-top-p-Nucleus-Sampling"><a href="#2-2-top-p-Nucleus-Sampling" class="headerlink" title="2.2. top-p (Nucleus Sampling)"></a>2.2. <code>top-p</code> (Nucleus Sampling)</h3><p><strong><code>top-p</code></strong> is the cumulative probability threshold for selecting words in nucleus sampling. Unlike top-k, it adapts dynamically to the output distribution.</p><ul><li><strong>Low <code>p</code> values</strong>: Lead to conservative text, similar to greedy decoding.</li><li><strong>High <code>p</code> values</strong>: Increase diversity by sampling from a larger set of possible words.</li></ul><p><strong>When to use?</strong>: Nucleus sampling is great for balancing quality and creativity, especially for conversational agents or storytelling.</p><h3 id="2-3-temperature"><a href="#2-3-temperature" class="headerlink" title="2.3. temperature"></a>2.3. <code>temperature</code></h3><p><strong><code>temperature</code></strong> affects the randomness of word selection by scaling the probabilities in the output distribution.</p><ul><li><strong>Lower temperature (e.g., 0.2)</strong>: More deterministic and focused outputs.</li><li><strong>Higher temperature (e.g., 1.2)</strong>: More creative but less predictable outputs.</li></ul><p><strong>When to use?</strong>: Adjust the temperature to match your needs. For factual, structured responses, a low temperature is preferred. For creative or open-ended generation, a higher temperature can lead to more varied responses.</p><h3 id="2-4-max-length"><a href="#2-4-max-length" class="headerlink" title="2.4. max_length"></a>2.4. <code>max_length</code></h3><p><strong><code>max_length</code></strong> defines the maximum number of tokens the model can generate. This limits how long the output will be and is often used to prevent overly long or runaway generations.</p><ul><li><strong>Short <code>max_length</code></strong>: Produces concise and to-the-point responses.</li><li><strong>Long <code>max_length</code></strong>: Useful for generating essays, articles, or other long-form content.</li></ul><hr><h2 id="3-How-Are-LLMs-Trained"><a href="#3-How-Are-LLMs-Trained" class="headerlink" title="3. How Are LLMs Trained?"></a>3. How Are LLMs Trained?</h2><p>Understanding decoding methods and parameters is critical for using LLMs effectively, but how are these models trained to generate such coherent and contextually accurate text? Let’s briefly cover the training process of LLMs.</p><h3 id="3-1-Pretraining-Phase"><a href="#3-1-Pretraining-Phase" class="headerlink" title="3.1. Pretraining Phase"></a>3.1. Pretraining Phase</h3><p>LLMs like GPT or BERT are trained on massive datasets containing text from the web, books, and other sources. During <strong>pretraining</strong>, the model learns to predict the next word in a sentence (for models like GPT) or predict masked words (for models like BERT). This helps the model develop a strong understanding of language patterns, syntax, and semantics.</p><h3 id="3-2-Fine-Tuning-Phase"><a href="#3-2-Fine-Tuning-Phase" class="headerlink" title="3.2. Fine-Tuning Phase"></a>3.2. Fine-Tuning Phase</h3><p>After pretraining, the model is <strong>fine-tuned</strong> on specific tasks, such as question answering, summarization, or text generation. Fine-tuning is done on a smaller, more specific dataset related to the task at hand. For instance, to fine-tune a model for customer service chatbots, it would be trained on dialogue data from customer interactions.</p><h3 id="3-3-Loss-Function"><a href="#3-3-Loss-Function" class="headerlink" title="3.3. Loss Function"></a>3.3. Loss Function</h3><p>The <strong>loss function</strong> during training helps the model learn from its mistakes. For LLMs, the most common loss function is <strong>cross-entropy loss</strong>, which measures how well the model’s predicted probability distribution matches the true distribution of the next word.</p><h3 id="3-4-Optimizers-and-Training-Dynamics"><a href="#3-4-Optimizers-and-Training-Dynamics" class="headerlink" title="3.4. Optimizers and Training Dynamics"></a>3.4. Optimizers and Training Dynamics</h3><p>To optimize the model’s parameters, techniques like <strong>Adam</strong> or <strong>AdamW</strong> (Weight Decay) are used. These algorithms help the model converge to a solution where the predictions match the real-world language distribution as closely as possible.</p><p>The training process involves:</p><ul><li><strong>Forward Pass</strong>: The model generates predictions.</li><li><strong>Backpropagation</strong>: Errors (measured by the loss function) are propagated back through the model to adjust its parameters.</li><li><strong>Gradient Descent</strong>: The optimizer updates the model’s weights to minimize the loss.</li></ul><hr><h2 id="4-Conclusion"><a href="#4-Conclusion" class="headerlink" title="4. Conclusion"></a>4. Conclusion</h2><p>The decoding phase of Large Language Models is where the raw power of the model is transformed into readable, meaningful text. By selecting the right decoding method (greedy, beam search, top-k, or top-p) and adjusting key parameters like <code>temperature</code>, <code>top-k</code>, and <code>top-p</code>, you can influence the creativity, coherence, and quality of the generated output.</p><p>Understanding these parameters allows you to fine-tune the model’s behavior for different tasks, from factual reporting to creative writing. Along with proper training, these methods ensure that LLMs deliver contextually accurate and relevant text.</p><hr><h2 id="5-Further-Reading"><a href="#5-Further-Reading" class="headerlink" title="5. Further Reading"></a>5. Further Reading</h2><ul><li><a href="https://huggingface.co/docs/transformers/v4.12.0/en/main_classes/text_generation">Hugging Face - Text Generation API</a></li><li><a href="https://beta.openai.com/docs/">OpenAI GPT-3 Documentation</a></li><li><a href="https://arxiv.org/abs/1904.09751">Top-k vs. Nucleus Sampling</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;When interacting with Large Language Models (LLMs) like GPT, BERT, or T5, the magic happens not only in the training but also in the &lt;str</summary>
      
    
    
    
    
    <category term="Large Language Model" scheme="https://stephen-smj.tech/tags/Large-Language-Model/"/>
    
  </entry>
  
  <entry>
    <title>经验分享-应届生如何在秋招拿下多个年薪50w的offer</title>
    <link href="https://stephen-smj.tech/2024/08/01/%E7%A7%8B%E6%8B%9B%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB/"/>
    <id>https://stephen-smj.tech/2024/08/01/%E7%A7%8B%E6%8B%9B%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB/</id>
    <published>2024-08-01T07:49:26.000Z</published>
    <updated>2024-08-10T08:54:06.778Z</updated>
    
    <content type="html"><![CDATA[<p>转眼间25届的秋招提前批已经开始啦，不禁让我想起了去年这个时间段边实习边秋招边完成毕业论文的煎熬。一直坚持到12月份之后感觉整体结果还是满意的。我同时参加了大陆的秋招和香港小部分公司的招聘（因为感觉香港没有分春秋招，全年可投），拿到的offer情况如下：</p><h3 id="大陆"><a href="#大陆" class="headerlink" title="大陆"></a>大陆</h3><p>投递了300多家公司，拿到多个互联网公司以及新能源公司的算法岗以及开发岗的offer。其中一个SSP，一个SP。</p><ol><li>SP来自国内某电商中偏大厂，AI开发工程师，给出x*16薪 + (房补+餐补)*12的offer总包刚好50w。</li><li>SSP来自国内某新能源Top1，大语言模型算法工程师，给出((x-8)*12) * 2的offer，比上一个差一些。（感觉是赶上风口了, 之前没想过自己能拿到纯算法岗的offer）</li></ol><p>还有一些其他互联网，能源，通信公司的offer相对这两个给的就很少，基本听到报价就直接拒了。</p><h3 id="香港"><a href="#香港" class="headerlink" title="香港"></a>香港</h3><ol><li><p>本地政府机构实习转正，给出（x-2）* 12 + 合同完期的bonus （两年总包*15%）。了解下来算是在香港本地公司里算比较高的了，但是跟大陆比还差一截。是的你没听错，<br>在寸土寸金的生活费巨高的香港给应届生的工资还不如大陆（It是这样的，其他行业感觉还是香港高一些）。</p></li><li><p>刚好赶上大陆某个Top1招聘行业的互联网公司出海，NLP算法岗，给出(x+1) * 14的总包，42w港币，后来我发现北京的总部也能拿到这个数的RMB。<br>后面问Hr，香港的生活成本比北京高多了，而且香港发的还是港币，相对发rmb的北京以后可能还要打八折，这么一算比北京还要少。Hr回复是根据香港本地It行情+30%来定价的<br>无异于再次证明了香港程序员的工资相对其他行业并不突出。其实我拿到这个offer的时候是想留下来的，但后来因为argue package搞得很不愉快，最终散是满天星了。</p></li></ol><p>后来偶然的机会让我选择了读博，于是乎就放弃了这些offer，感觉还是挺可惜的，毕竟大陆的互联网一年不如一年，现在还能喝口风口末期的汤，有朋友劝我说搞不好我博士毕业回去还不如硕士毕业拿得多，我也觉得有这种可能，但是能遇到一个欣赏我的Phd supervisor也是千载难逢。最后跟家人商量下来，还是觉得继续读书了。</p><h2 id="秋招准备经验分享"><a href="#秋招准备经验分享" class="headerlink" title="秋招准备经验分享"></a>秋招准备经验分享</h2><p>我在秋招总结出来了一套自己的准备方法，我觉得这个方法还是效率挺高的，分享出来希望能帮助到大家。</p><h3 id="刷题"><a href="#刷题" class="headerlink" title="刷题"></a>刷题</h3><p><img src="https://gist.github.com/user-attachments/assets/8c385ee2-c9d8-41ff-9aba-16de0603aad0" alt="image"></p><p>首先最基本的任务就是刷题了，互联网基本在3-6轮面试左右，前4轮基本每一轮都要当场手撕代码，写题大多是在牛客上，所以我后来觉得先刷LeetCode很不明智。于是从9月份开始我基本就只刷牛客上了，注意，LeetCode跟牛客的提交方式还是有很大差距，LeetCode是核心代码模式，不用自己写提交函数。而牛客是ACM模式，需要完整的代码。建议刚准备的同学可以先在LeetCode上刷一下常见题型，因为LeetCode比牛客要全一些，而且分类整理的更好。如果准备时间不够充分，就刷<a href="https://leetcode.cn/studyplan/top-100-liked/">hot100</a>。然后多去牛客上刷题，主要是为了熟练ACM模式的规则。整体上刷300道左右，面试写代码这一关通过率就很高了。</p><h3 id="八股文准备"><a href="#八股文准备" class="headerlink" title="八股文准备"></a>八股文准备</h3><p>技术面的第一个环节要么是写题，要么是八股文，题目没写出来八股文回答的好可能还能拯救一下，那么如何准备八股文呢。我的方法是自己整理一个跟自己用到的技术息息相关的文档，比如你简历里写了你熟练使用java，那面试官大概率会问你java里的锁机制，多线程，I/O，jvm等等（现在比较卷了，基础的继承，封装，多态这些基本不会再问了），大厂会问的特别深，很多问题必须看过源码才能回答上来，比如腾讯二面当时问我乐观锁和悲观锁有哪些，机制分别是什么样的，在我回答完synchronized锁之后又立马追问我synchronized锁是如何实现的。这些问题是背不完的，并且我觉得死记硬背也没意义，所以我建议应届生的简历里尽量不要用“精通”“熟练掌握”这些个词语，很多面试官自己都不敢这么写，看到你这么写之后就会有竞争心里，一定要问到你答不出来为止。再比如你简历里写了你懂计算机网络，那么面试官一定会问你TCP/UDP的通信过程，握手细节，http，https，websocket等协议的区别，拥塞控制和流量控制是如何实现的等等。<br><strong>所以简历里写到的所有技术的八股文一定要了解原理，不能只会用。</strong> 如果简历里写了却答不出来就会很扣分，并且会很尴尬。<br>这里我有一个小tips：<br>如果准备的时间不够充分，一定要把自己熟悉原理的技术放在前面，这样就可以起到吸引火力的作用，一般八股文这个环节就15分钟左右，问不了太多的问题。</p><h3 id="我所了解到的岗位需要准备的知识"><a href="#我所了解到的岗位需要准备的知识" class="headerlink" title="我所了解到的岗位需要准备的知识"></a>我所了解到的岗位需要准备的知识</h3><ul><li>后端开发岗必会：一个编程语言（一般是java或者go），数据结构，计算机网络，数据库（一般是MySQL）。</li><li>后端开发岗锦上添花（想在池子中排名靠前必会）：一个开发框架（SpringBoot，SpringCloud等），缓存数据库（Redis，MongoDB等）, 消息中间件（RabbitMQ，Kafka等等）。如果简历里写了这些，前面基础的问题可能就随便问一两个了。</li><li>前端开发岗必会（略微准备了一些，但没投过）：html，css，js三组件，一个开发框架（Vue或React），计算机网络。</li><li>算法岗必会：Pytorch/TensorFlow，你研究领域的常见算法，如做CV的必须会CNN以及一些变体，做NLP的必须精通Transformer，还有一些机器学习的算法原理，要能够很深入的回答出来某些结构为什么好为什么不好，比如序列任务中为什么batch normalization效果不好，以及某某损失函数在某个场景下为什么比其他好。</li><li>算法岗锦上添花：你这个领域近些年引用量比较高的论文里提出的方法得口到擒来，其次有个非常加分的点：了解一些工程性的技术，因为互联网大多部门的算法岗大部分工作其实在工程实现上，如果懂一些算法部署框架如Flask，加速框架如TensorRT等等会比较加分，能拿到SP的基本要求。</li></ul><h3 id="项目准备"><a href="#项目准备" class="headerlink" title="项目准备"></a>项目准备</h3><p>现在互联网比较卷了，大家基本都有多段实习，如果学历特别好只有一段拿得出手的实习就行。我准备项目的时候是这样准备的：<br>首先把你简历里所有项目经历放在一个文档里，比如Markdown，接下来对每一段项目经历总结以下几个点：</p><ol><li>技术原理：项目中用到的所有相关技术的八股文一定要非常了解。</li><li>复习项目：复习项目中的所有代码实现。如果没有代码了怎么办：至少能用一个非常通顺切合理的逻辑去把这个项目的实现过程复述一遍。</li><li>项目反问：反思项目中所有可能会被面试官问到的点。</li><li>项目贡献：最好能总结出来你在这段项目中带来了哪些贡献，比如系统的QPS经过你优化之后提升了多少多少，再比如高并发场景下你如何优化数据一致性问题。</li><li>改进方法：在你讲完你的项目之后，面试官可能还会追问针对你提到的某一个点，你还有没有改进的办法，所以提前把这些问题想好到时候就不会慌乱了。</li></ol><p><strong>尤其是第3点至今都让我觉得受益匪浅，我当时会不停的思考，我讲到任何一点的时候面试官都可能会打断我并且提出问题，那么我如何回答这个问题能让面试官非常满意。</strong></p><h4 id="这里给一些我当时整理的文档的截图，总共整理了10w多个字："><a href="#这里给一些我当时整理的文档的截图，总共整理了10w多个字：" class="headerlink" title="这里给一些我当时整理的文档的截图，总共整理了10w多个字："></a>这里给一些我当时整理的文档的截图，总共整理了10w多个字：</h4><h4 id="比如一个大模型的项目中："><a href="#比如一个大模型的项目中：" class="headerlink" title="比如一个大模型的项目中："></a>比如一个大模型的项目中：</h4><div style="width: 70%; text-align: center; margin: 0 auto"><img src="https://gist.github.com/user-attachments/assets/2820ed3d-0add-49d6-bc5c-3264ce61b8ae"><img src="https://gist.github.com/user-attachments/assets/fb0b819a-2480-4d26-8818-868743e6a010"></div><h4 id="再比如一个视觉算法的项目中："><a href="#再比如一个视觉算法的项目中：" class="headerlink" title="再比如一个视觉算法的项目中："></a>再比如一个视觉算法的项目中：</h4><div style="width: 70%; text-align: center; margin: 0 auto"><img src="https://gist.github.com/user-attachments/assets/fc3b4754-c506-4ce1-9950-45d33a225fb9"><img src="https://gist.github.com/user-attachments/assets/9e62360b-85ae-43d8-b98f-60a6a713ceb3"></div><h4 id="再比如一个开发的项目中："><a href="#再比如一个开发的项目中：" class="headerlink" title="再比如一个开发的项目中："></a>再比如一个开发的项目中：</h4><div style="width: 70%; text-align: center; margin: 0 auto"><img src="https://gist.github.com/user-attachments/assets/d2311966-b905-4ce5-8ce2-725827dfa555"></div><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>以上就是针对项目中的一些八股文和问题整理的回答思路。不需要整理的很规范，自己能看懂就行，节省时间用来投递最重要。<br>最后吐槽一下秋招投递：每一家公司要填的信息非常多，而且都是手填，有些插件效果也不太好，还是要自己调整，我最后就准备了一个专门用来投递的文档，把信息直接复制上去，不过还是很花时间，平均投递一家公司要20分钟。那么投递300家公司就是6000分钟，我当时秋招就差不多是这个数，后来因为有offer了且打算读书了，春招就直接放弃了。</p><h4 id="最后祝愿大家都能顺利上岸，拿到满意的Package！"><a href="#最后祝愿大家都能顺利上岸，拿到满意的Package！" class="headerlink" title="最后祝愿大家都能顺利上岸，拿到满意的Package！"></a>最后祝愿大家都能顺利上岸，拿到满意的Package！</h4>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;转眼间25届的秋招提前批已经开始啦，不禁让我想起了去年这个时间段边实习边秋招边完成毕业论文的煎熬。一直坚持到12月份之后感觉整体结果还是满意的。我同时参加了大陆的秋招和香港小部分公司的招聘（因为感觉香港没有分春秋招，全年可投），拿到的offer情况如下：&lt;/p&gt;
&lt;h3 i</summary>
      
    
    
    
    
    <category term="Experience" scheme="https://stephen-smj.tech/tags/Experience/"/>
    
  </entry>
  
  <entry>
    <title>What is an LLM Agent? A Guide to Large Language Model Agents</title>
    <link href="https://stephen-smj.tech/2024/07/23/What%20is%20LLM%20Agent/"/>
    <id>https://stephen-smj.tech/2024/07/23/What%20is%20LLM%20Agent/</id>
    <published>2024-07-23T08:04:38.000Z</published>
    <updated>2024-09-16T15:26:33.288Z</updated>
    
    <content type="html"><![CDATA[<p>Large Language Models (LLMs) have gained immense popularity due to their ability to generate, comprehend, and reason over text. But their usefulness goes beyond simple text generation—they can be designed as <strong>LLM Agents</strong> to autonomously perform tasks, interact with users, or even integrate with external systems.</p><p>In this blog, we’ll explore what LLM Agents are, how they function, and where they are being applied. To better understand the concept, we’ll also include a visual representation of an LLM Agent’s architecture.</p><h2 id="1-Introduction-to-LLM-Agents"><a href="#1-Introduction-to-LLM-Agents" class="headerlink" title="1. Introduction to LLM Agents"></a>1. Introduction to LLM Agents</h2><p>An <strong>LLM Agent</strong> is an autonomous system or “agent” powered by a large language model (LLM). These agents are designed to execute complex workflows, communicate with users, interact with APIs or tools, and make decisions based on natural language inputs. They often serve as bridges between users and external systems, processing language commands to complete tasks.</p><p>While a standard LLM is focused solely on generating human-like text, an LLM Agent extends this by incorporating elements such as reasoning, context, and interaction with tools.</p><h3 id="Key-Features-of-LLM-Agents"><a href="#Key-Features-of-LLM-Agents" class="headerlink" title="Key Features of LLM Agents:"></a>Key Features of LLM Agents:</h3><ul><li><strong>Task Automation</strong>: Can autonomously perform tasks based on natural language instructions.</li><li><strong>Reasoning</strong>: LLM Agents can reason over tasks, making decisions or completing multiple steps in a workflow.</li><li><strong>Tool Use</strong>: They can interact with APIs, databases, or other external tools to complete tasks.</li><li><strong>Conversational</strong>: These agents can maintain conversations, answer questions, and understand user intent.</li></ul><h2 id="2-How-Do-LLM-Agents-Work"><a href="#2-How-Do-LLM-Agents-Work" class="headerlink" title="2. How Do LLM Agents Work?"></a>2. How Do LLM Agents Work?</h2><p>An LLM Agent combines the power of a large language model (such as GPT, BERT, or others) with additional components that allow it to interact with the external world. </p><p>Here’s a typical flow of how an LLM Agent functions:</p><ol><li><strong>Input Parsing</strong>: The user provides an instruction or query, which the agent processes.</li><li><strong>LLM Reasoning</strong>: The agent’s core LLM analyzes the input and generates possible solutions or steps to complete the task.</li><li><strong>Tool Integration</strong>: If external data or actions are needed (e.g., searching a database or using an API), the agent can invoke specific tools.</li><li><strong>Execution</strong>: The agent performs the task, whether it’s answering a query, pulling data from an API, or executing multi-step workflows.</li><li><strong>Response Generation</strong>: After completing the task, the agent generates a final output (such as a text response or task confirmation) for the user.</li></ol><p>Below is a simple visual representation of an LLM Agent’s architecture:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">+------------------------+</span><br><span class="line">|      User Input         |</span><br><span class="line">+------------------------+</span><br><span class="line">          |</span><br><span class="line">          v</span><br><span class="line">+------------------------+</span><br><span class="line">|    LLM (Reasoning)      |</span><br><span class="line">+------------------------+</span><br><span class="line">          |</span><br><span class="line">          v</span><br><span class="line">+------------------------+        +-----------------------+</span><br><span class="line">|  Task Generation        |------&gt;| External Tools (APIs,  |</span><br><span class="line">+------------------------+        | Databases, Web Search) |</span><br><span class="line">          |                       +-----------------------+</span><br><span class="line">          v</span><br><span class="line">+------------------------+</span><br><span class="line">|  Task Execution         |</span><br><span class="line">+------------------------+</span><br><span class="line">          |</span><br><span class="line">          v</span><br><span class="line">+------------------------+</span><br><span class="line">|  Final Response         |</span><br><span class="line">+------------------------+</span><br></pre></td></tr></table></figure><h2 id="3-Example-of-an-LLM-Agent-in-Action"><a href="#3-Example-of-an-LLM-Agent-in-Action" class="headerlink" title="3. Example of an LLM Agent in Action"></a>3. Example of an LLM Agent in Action</h2><p>Imagine a scenario where you want to book a flight using natural language. Instead of manually searching for flights and filling out forms, an LLM Agent can autonomously handle the entire task. Here’s how:</p><h3 id="3-1-User-Input"><a href="#3-1-User-Input" class="headerlink" title="3.1. User Input"></a>3.1. User Input</h3><p>The user types in a natural language query like:</p><blockquote><p>“I need to book a flight from New York to San Francisco, departing on September 20th and returning on the 25th.”</p></blockquote><h3 id="3-2-LLM-Agent-Processing"><a href="#3-2-LLM-Agent-Processing" class="headerlink" title="3.2. LLM Agent Processing"></a>3.2. LLM Agent Processing</h3><ul><li>The <strong>LLM Agent</strong> parses the query to understand the user’s intent: “book a flight.”</li><li>It extracts key details such as departure city, destination city, and travel dates.</li></ul><h3 id="3-3-Tool-Integration"><a href="#3-3-Tool-Integration" class="headerlink" title="3.3. Tool Integration"></a>3.3. Tool Integration</h3><p>Next, the LLM Agent interacts with external tools:</p><ul><li><strong>Flight API</strong>: The agent searches for flights using an external flight API.</li><li><strong>Calendar API</strong>: It checks the user’s calendar to confirm availability.</li></ul><h3 id="3-4-Execution-and-Response"><a href="#3-4-Execution-and-Response" class="headerlink" title="3.4. Execution and Response"></a>3.4. Execution and Response</h3><p>Finally, the agent presents flight options to the user:</p><blockquote><p>“Here are three flights departing on September 20th from New York to San Francisco, with return flights on the 25th. Would you like to book one?”</p></blockquote><p>This kind of automation saves users from manually searching and handling bookings, offering a seamless experience.</p><h2 id="4-Applications-of-LLM-Agents"><a href="#4-Applications-of-LLM-Agents" class="headerlink" title="4. Applications of LLM Agents"></a>4. Applications of LLM Agents</h2><p>LLM Agents are transforming various industries by automating tasks that involve natural language understanding and execution. Here are some common applications:</p><h3 id="4-1-Virtual-Assistants"><a href="#4-1-Virtual-Assistants" class="headerlink" title="4.1. Virtual Assistants"></a>4.1. Virtual Assistants</h3><p>LLM Agents power virtual assistants like Amazon Alexa, Google Assistant, and Microsoft Cortana. These assistants can perform tasks such as sending reminders, setting alarms, answering questions, and more—all through voice commands or natural language.</p><h3 id="4-2-Customer-Support"><a href="#4-2-Customer-Support" class="headerlink" title="4.2. Customer Support"></a>4.2. Customer Support</h3><p>LLM Agents are used in customer support chatbots to autonomously respond to user inquiries, handle complaints, and even escalate issues to human agents when needed. These agents can improve customer satisfaction while reducing operational costs.</p><h3 id="4-3-Content-Generation"><a href="#4-3-Content-Generation" class="headerlink" title="4.3. Content Generation"></a>4.3. Content Generation</h3><p>From blog writing to automated report generation, LLM Agents can assist in content creation. By analyzing inputs like keywords or prompts, they generate coherent, human-like text across various domains.</p><h3 id="4-4-Data-Analysis"><a href="#4-4-Data-Analysis" class="headerlink" title="4.4. Data Analysis"></a>4.4. Data Analysis</h3><p>LLM Agents can autonomously interact with datasets, run queries, and return insights. For instance, they can be integrated with database systems to fetch relevant data based on user queries.</p><h3 id="4-5-Workflow-Automation"><a href="#4-5-Workflow-Automation" class="headerlink" title="4.5. Workflow Automation"></a>4.5. Workflow Automation</h3><p>In enterprise settings, LLM Agents can automate repetitive tasks like email sorting, document approvals, and data entry, increasing productivity by taking over mundane tasks.</p><h2 id="5-Conclusion"><a href="#5-Conclusion" class="headerlink" title="5. Conclusion"></a>5. Conclusion</h2><p>LLM Agents extend the capabilities of traditional large language models by making them more interactive, autonomous, and useful for real-world applications. With the ability to understand natural language, reason through tasks, and integrate with external tools, LLM Agents are shaping the future of human-computer interaction.</p><p>As these agents become more advanced, their use cases will expand, making them integral to industries such as customer service, automation, and personalized assistance.</p><h2 id="6-Further-Reading"><a href="#6-Further-Reading" class="headerlink" title="6. Further Reading"></a>6. Further Reading</h2><ul><li><a href="https://huggingface.co/transformers/">Hugging Face’s Transformers Library</a></li><li><a href="https://beta.openai.com/">OpenAI GPT Models</a></li><li><a href="https://en.wikipedia.org/wiki/Intelligent_agent">Introduction to Intelligent Agents</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Large Language Models (LLMs) have gained immense popularity due to their ability to generate, comprehend, and reason over text. But their</summary>
      
    
    
    
    
    <category term="Large Language Model" scheme="https://stephen-smj.tech/tags/Large-Language-Model/"/>
    
  </entry>
  
  <entry>
    <title>Introduction to Large Language Models (LLMs) and Their Principles</title>
    <link href="https://stephen-smj.tech/2024/07/09/LLM%20Introduction/"/>
    <id>https://stephen-smj.tech/2024/07/09/LLM%20Introduction/</id>
    <published>2024-07-09T08:04:38.000Z</published>
    <updated>2024-09-16T15:40:37.512Z</updated>
    
    <content type="html"><![CDATA[<p>Large Language Models (LLMs) are revolutionizing how machines understand, generate, and interact with human language. From powering chatbots to aiding in scientific research, LLMs have proven to be groundbreaking tools in natural language processing (NLP). In this blog post, we will delve into what LLMs are, how they work, and why they are so effective at generating human-like text.</p><h2 id="1-What-are-Large-Language-Models-LLMs"><a href="#1-What-are-Large-Language-Models-LLMs" class="headerlink" title="1. What are Large Language Models (LLMs)?"></a>1. What are Large Language Models (LLMs)?</h2><p>An <strong>LLM</strong> is a machine learning model designed to process and generate human language. They are typically built using deep learning techniques, specifically <strong>transformers</strong> (which we’ll explain later). These models are “large” because they consist of billions (or even trillions) of parameters that allow them to understand language context, structure, and semantics more deeply than traditional models.</p><h3 id="Key-Characteristics-of-LLMs"><a href="#Key-Characteristics-of-LLMs" class="headerlink" title="Key Characteristics of LLMs:"></a>Key Characteristics of LLMs:</h3><ul><li><strong>Scale</strong>: LLMs are trained on massive amounts of data, making them capable of understanding the nuances of human language.</li><li><strong>Multi-task Learning</strong>: They are versatile and can be used for multiple tasks such as text generation, summarization, translation, and even reasoning.</li><li><strong>Contextual Awareness</strong>: LLMs excel at understanding context, allowing them to generate coherent responses over long paragraphs.</li></ul><h3 id="Popular-LLMs"><a href="#Popular-LLMs" class="headerlink" title="Popular LLMs"></a>Popular LLMs</h3><p>Some widely used LLMs include:</p><ul><li><strong>GPT-3</strong> by OpenAI</li><li><strong>BERT</strong> by Google</li><li><strong>T5</strong> (Text-to-Text Transfer Transformer)</li><li><strong>BLOOM</strong> by BigScience</li></ul><h2 id="2-The-Inner-Workings-of-LLMs"><a href="#2-The-Inner-Workings-of-LLMs" class="headerlink" title="2. The Inner Workings of LLMs"></a>2. The Inner Workings of LLMs</h2><p>To understand how LLMs work, we need to examine the fundamental architecture behind them: <strong>transformers</strong>.</p><h3 id="2-1-Transformer-Architecture"><a href="#2-1-Transformer-Architecture" class="headerlink" title="2.1. Transformer Architecture"></a>2.1. Transformer Architecture</h3><p>The <strong>Transformer</strong> model, introduced in the paper <a href="https://arxiv.org/abs/1706.03762">“Attention is All You Need”</a> by Vaswani et al., forms the backbone of LLMs. It uses a mechanism called <strong>self-attention</strong> to focus on different parts of the input sequence and build relationships between words.</p><p>Below is a simplified illustration of the Transformer architecture:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">+-----------------------------------+</span><br><span class="line">|             Input Sequence        |</span><br><span class="line">+-----------------------------------+</span><br><span class="line">              |</span><br><span class="line">              v</span><br><span class="line">+-----------------------------------+</span><br><span class="line">|        Embedding Layer            |  </span><br><span class="line">+-----------------------------------+</span><br><span class="line">              |</span><br><span class="line">              v</span><br><span class="line">+-----------------------------------+</span><br><span class="line">|     Self-Attention Mechanism       |</span><br><span class="line">+-----------------------------------+</span><br><span class="line">              |</span><br><span class="line">              v</span><br><span class="line">+-----------------------------------+</span><br><span class="line">|      Feed-Forward Neural Network  |</span><br><span class="line">+-----------------------------------+</span><br><span class="line">              |</span><br><span class="line">              v</span><br><span class="line">+-----------------------------------+</span><br><span class="line">|           Output Sequence         |</span><br><span class="line">+-----------------------------------+</span><br></pre></td></tr></table></figure><h4 id="Key-Components-of-the-Transformer"><a href="#Key-Components-of-the-Transformer" class="headerlink" title="Key Components of the Transformer:"></a>Key Components of the Transformer:</h4><ul><li><strong>Embeddings</strong>: Each word or token in a sentence is transformed into a vector (embedding), which captures its meaning in a numerical form.</li><li><strong>Self-Attention</strong>: This is the heart of the transformer. It allows the model to focus on specific words in a sentence when making predictions, ensuring that context is taken into account. For example, in the sentence “She gave the book to her friend,” the model understands that “her” refers to “she” due to self-attention.</li><li><strong>Feed-Forward Networks</strong>: After applying self-attention, the transformer passes data through several fully connected neural networks to refine the prediction further.</li></ul><h3 id="2-2-Pretraining-and-Fine-Tuning"><a href="#2-2-Pretraining-and-Fine-Tuning" class="headerlink" title="2.2. Pretraining and Fine-Tuning"></a>2.2. Pretraining and Fine-Tuning</h3><p>LLMs are trained in two major stages:</p><h4 id="Pretraining"><a href="#Pretraining" class="headerlink" title="Pretraining"></a><strong>Pretraining</strong></h4><ul><li>During pretraining, the model is exposed to a massive amount of text data, and it learns to predict the next word in a sentence or fill in missing words. This phase helps the model develop a general understanding of language, context, and structure.</li><li>Example task: Given the sentence “The cat is on the ___,” the model learns to predict the word “mat.”</li></ul><h4 id="Fine-Tuning"><a href="#Fine-Tuning" class="headerlink" title="Fine-Tuning"></a><strong>Fine-Tuning</strong></h4><ul><li>After pretraining, LLMs are fine-tuned on specific tasks, such as translation or question-answering, by using labeled data. This specialization allows the model to excel in particular applications.</li></ul><h3 id="2-3-Large-Model-Size-and-Why-it-Matters"><a href="#2-3-Large-Model-Size-and-Why-it-Matters" class="headerlink" title="2.3. Large Model Size and Why it Matters"></a>2.3. Large Model Size and Why it Matters</h3><p>The “large” in LLM refers to the number of parameters in the model. Parameters are the internal weights that the model learns during training. The larger the number of parameters, the more capacity the model has to learn complex patterns in language.</p><p>For example:</p><ul><li><strong>GPT-3</strong> has 175 billion parameters.</li><li><strong>GPT-4</strong> (rumored to be even larger) further improves on GPT-3’s performance.</li></ul><p>The large scale of these models allows them to capture subtle patterns in language that smaller models miss, such as sarcasm, context switching, and idiomatic expressions.</p><h2 id="3-Why-Are-LLMs-So-Powerful"><a href="#3-Why-Are-LLMs-So-Powerful" class="headerlink" title="3. Why Are LLMs So Powerful?"></a>3. Why Are LLMs So Powerful?</h2><h3 id="3-1-Contextual-Understanding"><a href="#3-1-Contextual-Understanding" class="headerlink" title="3.1. Contextual Understanding"></a>3.1. Contextual Understanding</h3><p>One of the biggest strengths of LLMs is their ability to understand the context of a conversation or text passage. Unlike traditional models that handle one word or sentence at a time, LLMs can process entire paragraphs and take into account what has been previously mentioned. This is crucial for tasks like long-form text generation, summarization, and chat-based applications.</p><h3 id="3-2-Transfer-Learning"><a href="#3-2-Transfer-Learning" class="headerlink" title="3.2. Transfer Learning"></a>3.2. Transfer Learning</h3><p>LLMs leverage <strong>transfer learning</strong>, meaning they are trained on massive datasets and then fine-tuned for specific tasks. This allows a single model to perform multiple tasks with impressive accuracy. For instance, GPT-3 can write essays, translate text, answer questions, and even write code—all without being explicitly programmed for each of these tasks.</p><h3 id="3-3-Multilingual-Capabilities"><a href="#3-3-Multilingual-Capabilities" class="headerlink" title="3.3. Multilingual Capabilities"></a>3.3. Multilingual Capabilities</h3><p>Many LLMs are trained on text data from multiple languages, giving them the ability to understand and generate text in different languages. For example, models like <strong>mBERT</strong> (Multilingual BERT) can process over 100 languages.</p><h3 id="3-4-Generalization"><a href="#3-4-Generalization" class="headerlink" title="3.4. Generalization"></a>3.4. Generalization</h3><p>LLMs are not limited to just one domain. They can perform well across multiple industries—be it legal text, scientific research, literature, or social media. This generalization makes them incredibly versatile tools.</p><h2 id="4-Applications-of-LLMs"><a href="#4-Applications-of-LLMs" class="headerlink" title="4. Applications of LLMs"></a>4. Applications of LLMs</h2><p>LLMs have a wide range of applications in various industries. Here are a few of the most impactful ones:</p><h3 id="4-1-Content-Generation"><a href="#4-1-Content-Generation" class="headerlink" title="4.1. Content Generation"></a>4.1. Content Generation</h3><p>LLMs are widely used for text generation tasks such as writing articles, reports, and even creative writing. Applications like OpenAI’s GPT-3 can write essays, product descriptions, or even poetry with minimal human input.</p><h3 id="4-2-Chatbots-and-Virtual-Assistants"><a href="#4-2-Chatbots-and-Virtual-Assistants" class="headerlink" title="4.2. Chatbots and Virtual Assistants"></a>4.2. Chatbots and Virtual Assistants</h3><p>One of the most popular applications of LLMs is in chatbots and virtual assistants. These systems can understand natural language commands, carry on conversations, and assist users with a wide range of tasks—from customer service to personal assistance.</p><h3 id="4-3-Code-Generation"><a href="#4-3-Code-Generation" class="headerlink" title="4.3. Code Generation"></a>4.3. Code Generation</h3><p>LLMs like GitHub Copilot (powered by GPT) have been used to generate code snippets based on natural language descriptions, helping developers write code faster and with fewer errors.</p><h3 id="4-4-Translation-and-Summarization"><a href="#4-4-Translation-and-Summarization" class="headerlink" title="4.4. Translation and Summarization"></a>4.4. Translation and Summarization</h3><p>LLMs excel at machine translation, converting text from one language to another. They also perform well at summarizing long passages of text into shorter, more concise versions, making them valuable for tasks such as document summarization and news aggregation.</p><h2 id="5-Challenges-and-Limitations-of-LLMs"><a href="#5-Challenges-and-Limitations-of-LLMs" class="headerlink" title="5. Challenges and Limitations of LLMs"></a>5. Challenges and Limitations of LLMs</h2><p>While LLMs are incredibly powerful, they also come with certain challenges:</p><h3 id="5-1-Computational-Resources"><a href="#5-1-Computational-Resources" class="headerlink" title="5.1. Computational Resources"></a>5.1. Computational Resources</h3><p>Training LLMs requires enormous computational power and memory, making them resource-intensive. Large organizations like OpenAI, Google, and Microsoft have the infrastructure to handle this, but it’s a barrier for smaller companies and independent researchers.</p><h3 id="5-2-Ethical-Concerns"><a href="#5-2-Ethical-Concerns" class="headerlink" title="5.2. Ethical Concerns"></a>5.2. Ethical Concerns</h3><p>LLMs can sometimes generate biased or harmful outputs because they are trained on real-world data, which may contain biased, toxic, or offensive content. Ensuring fairness and preventing harmful outcomes is an ongoing challenge.</p><h3 id="5-3-Lack-of-True-Understanding"><a href="#5-3-Lack-of-True-Understanding" class="headerlink" title="5.3. Lack of True Understanding"></a>5.3. Lack of True Understanding</h3><p>While LLMs can generate human-like text, they do not “understand” language the way humans do. They lack common sense reasoning and may provide nonsensical or misleading answers in certain contexts.</p><h2 id="6-Conclusion"><a href="#6-Conclusion" class="headerlink" title="6. Conclusion"></a>6. Conclusion</h2><p>Large Language Models have transformed natural language processing, enabling machines to interact with humans more naturally than ever before. Their ability to understand context, generate coherent text, and perform various tasks makes them versatile and powerful tools. However, their challenges—such as resource requirements and ethical concerns—highlight the need for responsible development and deployment.</p><p>As research continues to push the boundaries of LLMs, we can expect even more sophisticated models that will further enhance AI’s role in our daily lives.</p><h2 id="7-Further-Reading"><a href="#7-Further-Reading" class="headerlink" title="7. Further Reading"></a>7. Further Reading</h2><ul><li><a href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a></li><li><a href="https://beta.openai.com/docs/">GPT-3 by OpenAI</a></li><li><a href="https://arxiv.org/abs/1810.04805">Google’s BERT</a></li><li><a href="https://huggingface.co/transformers/">Hugging Face: Transformers Library</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Large Language Models (LLMs) are revolutionizing how machines understand, generate, and interact with human language. From powering chatb</summary>
      
    
    
    
    
    <category term="Large Language Model" scheme="https://stephen-smj.tech/tags/Large-Language-Model/"/>
    
  </entry>
  
  <entry>
    <title>Essential PyTorch Syntax for Deep Learning - A Quick Guide</title>
    <link href="https://stephen-smj.tech/2024/06/23/Pytorch/"/>
    <id>https://stephen-smj.tech/2024/06/23/Pytorch/</id>
    <published>2024-06-23T03:56:06.000Z</published>
    <updated>2024-09-16T15:40:18.569Z</updated>
    
    <content type="html"><![CDATA[<p>PyTorch is one of the most popular deep learning frameworks due to its ease of use, flexibility, and dynamic computational graph. If you’re new to PyTorch or need a quick reference, this guide will walk you through the most commonly used syntax and commands.</p><h2 id="1-What-is-PyTorch"><a href="#1-What-is-PyTorch" class="headerlink" title="1. What is PyTorch?"></a>1. What is PyTorch?</h2><p>PyTorch is an open-source deep learning framework developed by Facebook’s AI Research lab (FAIR). It allows users to build complex neural networks with a Pythonic approach, making it accessible for both research and production.</p><h2 id="2-Installing-PyTorch"><a href="#2-Installing-PyTorch" class="headerlink" title="2. Installing PyTorch"></a>2. Installing PyTorch</h2><p>First, you need to install PyTorch. Depending on your system configuration (CPU/GPU), you can install PyTorch with:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># For CPU</span></span><br><span class="line">pip install torch torchvision</span><br><span class="line"></span><br><span class="line"><span class="comment"># For GPU (CUDA 11.8 as an example)</span></span><br><span class="line">pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118</span><br></pre></td></tr></table></figure><p>Now, let’s dive into the most frequently used PyTorch syntax.</p><h2 id="3-Basic-Tensor-Operations"><a href="#3-Basic-Tensor-Operations" class="headerlink" title="3. Basic Tensor Operations"></a>3. Basic Tensor Operations</h2><p>Tensors are the core data structure in PyTorch, and they are similar to NumPy arrays. Let’s start by creating and manipulating tensors.</p><h3 id="3-1-Creating-Tensors"><a href="#3-1-Creating-Tensors" class="headerlink" title="3.1. Creating Tensors"></a>3.1. Creating Tensors</h3><p>You can create tensors using various methods:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># Creating a tensor from a list</span></span><br><span class="line">x = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Creating a tensor with all zeros</span></span><br><span class="line">zeros = torch.zeros((<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line"><span class="built_in">print</span>(zeros)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Creating a tensor with random values</span></span><br><span class="line">rand_tensor = torch.rand((<span class="number">2</span>, <span class="number">4</span>))</span><br><span class="line"><span class="built_in">print</span>(rand_tensor)</span><br></pre></td></tr></table></figure><h3 id="3-2-Tensor-Operations"><a href="#3-2-Tensor-Operations" class="headerlink" title="3.2. Tensor Operations"></a>3.2. Tensor Operations</h3><p>You can perform a variety of operations on tensors, including addition, multiplication, and reshaping:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Element-wise addition</span></span><br><span class="line">a = torch.tensor([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">b = torch.tensor([<span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">c = a + b</span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Matrix multiplication</span></span><br><span class="line">mat1 = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">mat2 = torch.tensor([[<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]])</span><br><span class="line">result = torch.matmul(mat1, mat2)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Reshaping a tensor</span></span><br><span class="line">reshaped = mat1.view(<span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(reshaped)</span><br></pre></td></tr></table></figure><h3 id="3-3-Moving-Tensors-to-GPU"><a href="#3-3-Moving-Tensors-to-GPU" class="headerlink" title="3.3. Moving Tensors to GPU"></a>3.3. Moving Tensors to GPU</h3><p>If you’re working with large models, you’ll likely want to take advantage of a GPU:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Check if CUDA (GPU) is available</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Move tensor to GPU</span></span><br><span class="line">gpu_tensor = torch.rand((<span class="number">2</span>, <span class="number">2</span>)).to(device)</span><br><span class="line"><span class="built_in">print</span>(gpu_tensor)</span><br></pre></td></tr></table></figure><h2 id="4-Building-a-Simple-Neural-Network"><a href="#4-Building-a-Simple-Neural-Network" class="headerlink" title="4. Building a Simple Neural Network"></a>4. Building a Simple Neural Network</h2><p>Let’s see how to define and train a simple neural network using PyTorch’s <code>nn.Module</code> class. </p><h3 id="4-1-Defining-a-Model"><a href="#4-1-Defining-a-Model" class="headerlink" title="4.1. Defining a Model"></a>4.1. Defining a Model</h3><p>In PyTorch, you define your neural network as a class that inherits from <code>nn.Module</code>. Here’s an example of a basic fully connected neural network:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNN, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">28</span>*<span class="number">28</span>, <span class="number">128</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">128</span>, <span class="number">64</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h3 id="4-2-Initializing-and-Using-the-Model"><a href="#4-2-Initializing-and-Using-the-Model" class="headerlink" title="4.2. Initializing and Using the Model"></a>4.2. Initializing and Using the Model</h3><p>To initialize the model and pass data through it, use the following commands:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">model = SimpleNN()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Random input tensor simulating a batch of 32 images, each of size 28x28</span></span><br><span class="line">input_data = torch.rand((<span class="number">32</span>, <span class="number">28</span>*<span class="number">28</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Forward pass</span></span><br><span class="line">output = model(input_data)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure><h2 id="5-Loss-Functions-and-Optimizers"><a href="#5-Loss-Functions-and-Optimizers" class="headerlink" title="5. Loss Functions and Optimizers"></a>5. Loss Functions and Optimizers</h2><p>PyTorch provides various built-in loss functions and optimizers. Let’s use Cross-Entropy Loss and Stochastic Gradient Descent (SGD) for a classification task.</p><h3 id="5-1-Defining-a-Loss-Function"><a href="#5-1-Defining-a-Loss-Function" class="headerlink" title="5.1. Defining a Loss Function"></a>5.1. Defining a Loss Function</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">criterion = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure><h3 id="5-2-Optimizer-Setup"><a href="#5-2-Optimizer-Setup" class="headerlink" title="5.2. Optimizer Setup"></a>5.2. Optimizer Setup</h3><p>To update the model weights, you can use an optimizer like <code>torch.optim.SGD</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure><h3 id="5-3-Training-the-Model"><a href="#5-3-Training-the-Model" class="headerlink" title="5.3. Training the Model"></a>5.3. Training the Model</h3><p>Here’s how you would structure the training loop in PyTorch:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Sample input and target</span></span><br><span class="line">input_data = torch.rand((<span class="number">32</span>, <span class="number">28</span>*<span class="number">28</span>))</span><br><span class="line">target = torch.randint(<span class="number">0</span>, <span class="number">10</span>, (<span class="number">32</span>,))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Zero the gradients</span></span><br><span class="line">optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Forward pass</span></span><br><span class="line">output = model(input_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate loss</span></span><br><span class="line">loss = criterion(output, target)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Backward pass (compute gradients)</span></span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Update weights</span></span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure><h2 id="6-DataLoader-Loading-Data-Efficiently"><a href="#6-DataLoader-Loading-Data-Efficiently" class="headerlink" title="6. DataLoader: Loading Data Efficiently"></a>6. DataLoader: Loading Data Efficiently</h2><p>PyTorch’s <code>DataLoader</code> is essential for handling large datasets, especially when dealing with mini-batch training.</p><h3 id="6-1-Using-DataLoader-with-Built-in-Datasets"><a href="#6-1-Using-DataLoader-with-Built-in-Datasets" class="headerlink" title="6.1. Using DataLoader with Built-in Datasets"></a>6.1. Using DataLoader with Built-in Datasets</h3><p>PyTorch provides several built-in datasets, such as MNIST. You can easily load these datasets and use a DataLoader for batching:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># Transform to normalize data and convert to tensor</span></span><br><span class="line">transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Download the MNIST dataset</span></span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, transform=transform, download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the dataset into DataLoader</span></span><br><span class="line">train_loader = DataLoader(train_dataset, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Iterating through the DataLoader</span></span><br><span class="line"><span class="keyword">for</span> batch_idx, (data, target) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">    <span class="built_in">print</span>(batch_idx, data.size(), target.size())</span><br></pre></td></tr></table></figure><h3 id="6-2-Custom-Dataset"><a href="#6-2-Custom-Dataset" class="headerlink" title="6.2. Custom Dataset"></a>6.2. Custom Dataset</h3><p>You can also create custom datasets by subclassing <code>torch.utils.data.Dataset</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CustomDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data, labels</span>):</span><br><span class="line">        self.data = data</span><br><span class="line">        self.labels = labels</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> self.data[idx], self.labels[idx]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Example usage</span></span><br><span class="line">custom_data = CustomDataset(torch.rand((<span class="number">100</span>, <span class="number">28</span>*<span class="number">28</span>)), torch.randint(<span class="number">0</span>, <span class="number">10</span>, (<span class="number">100</span>,)))</span><br><span class="line">custom_loader = DataLoader(custom_data, batch_size=<span class="number">16</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h2 id="7-Conclusion"><a href="#7-Conclusion" class="headerlink" title="7. Conclusion"></a>7. Conclusion</h2><p>In this post, we’ve covered the most commonly used syntax in PyTorch, including basic tensor operations, building neural networks, handling loss functions and optimizers, and working with DataLoader. These essential tools provide the foundation for building deep learning models efficiently in PyTorch.</p><p>PyTorch’s flexibility makes it great for experimentation, while its performance makes it suitable for production environments as well. Whether you’re training a small neural network or a large transformer model, PyTorch’s intuitive API makes it a valuable tool for deep learning.</p><h2 id="8-Further-Reading"><a href="#8-Further-Reading" class="headerlink" title="8. Further Reading"></a>8. Further Reading</h2><ul><li><a href="https://pytorch.org/docs/stable/index.html">PyTorch Documentation</a></li><li><a href="https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html">Deep Learning with PyTorch</a></li><li><a href="https://pytorch.org/vision/stable/index.html">Torchvision: Datasets and Models for Computer Vision</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;PyTorch is one of the most popular deep learning frameworks due to its ease of use, flexibility, and dynamic computational graph. If you’</summary>
      
    
    
    
    
    <category term="Deep learning" scheme="https://stephen-smj.tech/tags/Deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>Mastering the Hugging Face Transformers Library - A Quick Introduction</title>
    <link href="https://stephen-smj.tech/2024/05/25/Transformers/"/>
    <id>https://stephen-smj.tech/2024/05/25/Transformers/</id>
    <published>2024-05-25T15:09:34.000Z</published>
    <updated>2024-09-16T15:40:12.095Z</updated>
    
    <content type="html"><![CDATA[<p>Transformers have revolutionized the field of Natural Language Processing (NLP), and Hugging Face’s <strong>Transformers</strong> library makes it easy to access state-of-the-art models. Whether you’re dealing with text classification, question-answering, or even text generation, the Transformers library simplifies model deployment and usage.</p><p>In this blog, we’ll walk through the basic usage of the Hugging Face Transformers library and showcase how to fine-tune pre-trained models for specific NLP tasks.</p><h2 id="1-What-is-the-Hugging-Face-Transformers-Library"><a href="#1-What-is-the-Hugging-Face-Transformers-Library" class="headerlink" title="1. What is the Hugging Face Transformers Library?"></a>1. What is the Hugging Face Transformers Library?</h2><p>The Hugging Face <strong>Transformers</strong> library is an open-source Python package that provides easy access to over 50,000 pre-trained models for a wide variety of NLP tasks. The library includes models like BERT, GPT, RoBERTa, T5, and more, enabling developers to quickly deploy solutions without needing to train models from scratch.</p><h2 id="2-Installing-the-Transformers-Library"><a href="#2-Installing-the-Transformers-Library" class="headerlink" title="2. Installing the Transformers Library"></a>2. Installing the Transformers Library</h2><p>Before diving into code, you’ll need to install the Transformers library. You can do this with pip:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install transformers</span><br></pre></td></tr></table></figure><p>You might also want to install PyTorch or TensorFlow, depending on the framework you’re comfortable with:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># For PyTorch</span></span><br><span class="line">pip install torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># For TensorFlow</span></span><br><span class="line">pip install tensorflow</span><br></pre></td></tr></table></figure><h2 id="3-Loading-Pre-trained-Models-and-Tokenizers"><a href="#3-Loading-Pre-trained-Models-and-Tokenizers" class="headerlink" title="3. Loading Pre-trained Models and Tokenizers"></a>3. Loading Pre-trained Models and Tokenizers</h2><p>One of the most powerful features of the <strong>Transformers</strong> library is the ability to load pre-trained models with just a few lines of code. Let’s load BERT and its corresponding tokenizer.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, BertModel</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load pre-trained model tokenizer</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">&#x27;bert-base-uncased&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load pre-trained BERT model</span></span><br><span class="line">model = BertModel.from_pretrained(<span class="string">&#x27;bert-base-uncased&#x27;</span>)</span><br></pre></td></tr></table></figure><h3 id="3-1-Tokenizing-Input-Text"><a href="#3-1-Tokenizing-Input-Text" class="headerlink" title="3.1. Tokenizing Input Text"></a>3.1. Tokenizing Input Text</h3><p>Tokenization is the first step in preparing text for model input. The tokenizer breaks the text into tokens and encodes it into a format the model can understand:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">text = <span class="string">&quot;Hugging Face makes NLP easy!&quot;</span></span><br><span class="line">inputs = tokenizer(text, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(inputs)</span><br></pre></td></tr></table></figure><p>The tokenizer returns input IDs and attention masks, which will be fed into the model.</p><h3 id="3-2-Generating-Model-Outputs"><a href="#3-2-Generating-Model-Outputs" class="headerlink" title="3.2. Generating Model Outputs"></a>3.2. Generating Model Outputs</h3><p>Once you have tokenized the input, you can pass it to the model:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">outputs = model(**inputs)</span><br><span class="line"><span class="built_in">print</span>(outputs.last_hidden_state)</span><br></pre></td></tr></table></figure><p>The model output includes hidden states and other useful information, which can be processed for downstream tasks like classification or text generation.</p><h2 id="4-Fine-tuning-a-Model-for-Text-Classification"><a href="#4-Fine-tuning-a-Model-for-Text-Classification" class="headerlink" title="4. Fine-tuning a Model for Text Classification"></a>4. Fine-tuning a Model for Text Classification</h2><p>Fine-tuning pre-trained models for specific tasks like text classification is a common use case. Here’s how you can fine-tune a BERT model for sentiment analysis.</p><h3 id="4-1-Dataset-Preparation"><a href="#4-1-Dataset-Preparation" class="headerlink" title="4.1. Dataset Preparation"></a>4.1. Dataset Preparation</h3><p>We’ll use a dataset from Hugging Face’s <strong>datasets</strong> library. First, install the library if you haven’t:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install datasets</span><br></pre></td></tr></table></figure><p>Then, load a dataset:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line">dataset = load_dataset(<span class="string">&#x27;imdb&#x27;</span>)</span><br></pre></td></tr></table></figure><h3 id="4-2-Preparing-Data-for-Training"><a href="#4-2-Preparing-Data-for-Training" class="headerlink" title="4.2. Preparing Data for Training"></a>4.2. Preparing Data for Training</h3><p>The <strong>datasets</strong> library integrates smoothly with the <strong>Transformers</strong> library, making it easy to prepare the data for training:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess_data</span>(<span class="params">examples</span>):</span><br><span class="line">    <span class="keyword">return</span> tokenizer(examples[<span class="string">&#x27;text&#x27;</span>], truncation=<span class="literal">True</span>, padding=<span class="string">&#x27;max_length&#x27;</span>)</span><br><span class="line"></span><br><span class="line">encoded_dataset = dataset.<span class="built_in">map</span>(preprocess_data, batched=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h3 id="4-3-Fine-tuning-the-Model"><a href="#4-3-Fine-tuning-the-Model" class="headerlink" title="4.3. Fine-tuning the Model"></a>4.3. Fine-tuning the Model</h3><p>You can now fine-tune a pre-trained model like BERT using the <code>Trainer</code> class:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertForSequenceClassification, Trainer, TrainingArguments</span><br><span class="line"></span><br><span class="line">model = BertForSequenceClassification.from_pretrained(<span class="string">&#x27;bert-base-uncased&#x27;</span>, num_labels=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">training_args = TrainingArguments(</span><br><span class="line">    output_dir=<span class="string">&#x27;./results&#x27;</span>,</span><br><span class="line">    evaluation_strategy=<span class="string">&quot;epoch&quot;</span>,</span><br><span class="line">    per_device_train_batch_size=<span class="number">16</span>,</span><br><span class="line">    per_device_eval_batch_size=<span class="number">16</span>,</span><br><span class="line">    num_train_epochs=<span class="number">3</span>,</span><br><span class="line">    weight_decay=<span class="number">0.01</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model,</span><br><span class="line">    args=training_args,</span><br><span class="line">    train_dataset=encoded_dataset[<span class="string">&#x27;train&#x27;</span>],</span><br><span class="line">    eval_dataset=encoded_dataset[<span class="string">&#x27;test&#x27;</span>]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer.train()</span><br></pre></td></tr></table></figure><p>After fine-tuning, you can use the model for inference or further evaluation.</p><h2 id="5-Saving-and-Loading-Fine-Tuned-Models"><a href="#5-Saving-and-Loading-Fine-Tuned-Models" class="headerlink" title="5. Saving and Loading Fine-Tuned Models"></a>5. Saving and Loading Fine-Tuned Models</h2><p>Once fine-tuning is complete, you can save the model and tokenizer for later use:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model.save_pretrained(<span class="string">&#x27;./fine-tuned-bert&#x27;</span>)</span><br><span class="line">tokenizer.save_pretrained(<span class="string">&#x27;./fine-tuned-bert&#x27;</span>)</span><br></pre></td></tr></table></figure><p>To load the model and tokenizer back:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = BertForSequenceClassification.from_pretrained(<span class="string">&#x27;./fine-tuned-bert&#x27;</span>)</span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">&#x27;./fine-tuned-bert&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="6-Conclusion"><a href="#6-Conclusion" class="headerlink" title="6. Conclusion"></a>6. Conclusion</h2><p>In this post, we’ve covered the basics of using the Hugging Face <strong>Transformers</strong> library, including loading pre-trained models, tokenizing text, and fine-tuning models for specific tasks. The <strong>Transformers</strong> library provides an intuitive and powerful interface to leverage state-of-the-art models without the complexity of training from scratch.</p><p>If you’re looking to dive deeper, there are plenty of additional features like model distillation, multi-task learning, and more. Happy coding!</p><h2 id="7-Further-Reading"><a href="#7-Further-Reading" class="headerlink" title="7. Further Reading"></a>7. Further Reading</h2><ul><li><a href="https://huggingface.co/docs/transformers">Hugging Face Transformers Documentation</a></li><li><a href="https://huggingface.co/models">Transformers Model Hub</a></li><li><a href="https://huggingface.co/docs/datasets">Datasets Documentation</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Transformers have revolutionized the field of Natural Language Processing (NLP), and Hugging Face’s &lt;strong&gt;Transformers&lt;/strong&gt; library</summary>
      
    
    
    
    
    <category term="Deep learning" scheme="https://stephen-smj.tech/tags/Deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>Getting Started with LangChain - A Beginner’s Guide</title>
    <link href="https://stephen-smj.tech/2024/05/01/Start%20with%20LangChain/"/>
    <id>https://stephen-smj.tech/2024/05/01/Start%20with%20LangChain/</id>
    <published>2024-05-01T15:09:34.000Z</published>
    <updated>2024-09-16T15:38:26.207Z</updated>
    
    <content type="html"><![CDATA[<p>LangChain is a powerful library designed to facilitate the creation and management of complex language models and pipelines. Whether you’re a data scientist, a machine learning engineer, or just someone interested in natural language processing, LangChain offers tools and abstractions that can simplify your work.</p><p>In this blog post, we’ll cover the basics of using LangChain, including setting up your environment, creating a simple language model pipeline, and running some basic operations. By the end, you’ll have a good understanding of how to get started with LangChain.</p><h2 id="1-Introduction-to-LangChain"><a href="#1-Introduction-to-LangChain" class="headerlink" title="1. Introduction to LangChain"></a>1. Introduction to LangChain</h2><p>LangChain is built to streamline the integration of large language models (LLMs) into various applications. It provides a flexible and modular framework that allows you to build custom pipelines for processing and generating text.</p><h2 id="2-Setting-Up-Your-Environment"><a href="#2-Setting-Up-Your-Environment" class="headerlink" title="2. Setting Up Your Environment"></a>2. Setting Up Your Environment</h2><p>Before you can start using LangChain, you need to set up your development environment. Here’s a step-by-step guide:</p><h3 id="2-1-Install-LangChain"><a href="#2-1-Install-LangChain" class="headerlink" title="2.1. Install LangChain"></a>2.1. Install LangChain</h3><p>First, you’ll need to install the LangChain library. You can do this using pip:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install langchain</span><br></pre></td></tr></table></figure><h3 id="2-2-Install-Additional-Dependencies"><a href="#2-2-Install-Additional-Dependencies" class="headerlink" title="2.2. Install Additional Dependencies"></a>2.2. Install Additional Dependencies</h3><p>LangChain may also require other dependencies depending on the features you plan to use. For instance, if you want to work with specific language models, you might need to install additional packages.</p><h2 id="3-Creating-a-Simple-Language-Model-Pipeline"><a href="#3-Creating-a-Simple-Language-Model-Pipeline" class="headerlink" title="3. Creating a Simple Language Model Pipeline"></a>3. Creating a Simple Language Model Pipeline</h2><p>Let’s create a basic pipeline using LangChain to process text. This example will demonstrate how to initialize a language model and perform text generation.</p><h3 id="3-1-Import-LangChain"><a href="#3-1-Import-LangChain" class="headerlink" title="3.1. Import LangChain"></a>3.1. Import LangChain</h3><p>Start by importing the necessary components from LangChain:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain <span class="keyword">import</span> LanguageModel, Pipeline</span><br></pre></td></tr></table></figure><h3 id="3-2-Initialize-the-Language-Model"><a href="#3-2-Initialize-the-Language-Model" class="headerlink" title="3.2. Initialize the Language Model"></a>3.2. Initialize the Language Model</h3><p>You can use a pre-trained language model or create a custom one. For simplicity, we’ll use a generic model:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = LanguageModel(model_name=<span class="string">&quot;gpt-3.5-turbo&quot;</span>)</span><br></pre></td></tr></table></figure><h3 id="3-3-Create-a-Pipeline"><a href="#3-3-Create-a-Pipeline" class="headerlink" title="3.3. Create a Pipeline"></a>3.3. Create a Pipeline</h3><p>Create a pipeline that includes your language model and defines how text will be processed:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pipeline = Pipeline(steps=[model])</span><br></pre></td></tr></table></figure><h3 id="3-4-Run-the-Pipeline"><a href="#3-4-Run-the-Pipeline" class="headerlink" title="3.4. Run the Pipeline"></a>3.4. Run the Pipeline</h3><p>Process a sample input text through the pipeline:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">input_text = <span class="string">&quot;What is LangChain?&quot;</span></span><br><span class="line">output_text = pipeline.run(input_text)</span><br><span class="line"><span class="built_in">print</span>(output_text)</span><br></pre></td></tr></table></figure><h2 id="4-Visualizing-the-Pipeline"><a href="#4-Visualizing-the-Pipeline" class="headerlink" title="4. Visualizing the Pipeline"></a>4. Visualizing the Pipeline</h2><p>LangChain allows for visualization of pipelines, which can help in understanding the flow of data. You can use tools like Graphviz to visualize your pipeline:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.visualization <span class="keyword">import</span> visualize_pipeline</span><br><span class="line"></span><br><span class="line">visualize_pipeline(pipeline, filename=<span class="string">&quot;pipeline_graph.png&quot;</span>)</span><br></pre></td></tr></table></figure><p>Ensure you have Graphviz installed:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install graphviz</span><br></pre></td></tr></table></figure><h2 id="5-Conclusion"><a href="#5-Conclusion" class="headerlink" title="5. Conclusion"></a>5. Conclusion</h2><p>In this tutorial, we covered the basics of getting started with LangChain. You learned how to set up your environment, create a simple language model pipeline, and visualize it. LangChain offers much more functionality, and exploring its documentation can help you build more complex and powerful text processing applications.</p><p>Stay tuned for more detailed tutorials on advanced features of LangChain, such as custom model integration, advanced pipeline configurations, and more.</p><h2 id="6-Further-Reading"><a href="#6-Further-Reading" class="headerlink" title="6. Further Reading"></a>6. Further Reading</h2><ul><li><a href="https://langchain.com/docs">LangChain Documentation</a></li><li><a href="https://graphviz.gitlab.io/documentation/">Graphviz Visualization Guide</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;LangChain is a powerful library designed to facilitate the creation and management of complex language models and pipelines. Whether you’</summary>
      
    
    
    
    
    <category term="Large Language Model" scheme="https://stephen-smj.tech/tags/Large-Language-Model/"/>
    
  </entry>
  
  <entry>
    <title>Achieve Function Calling and Tool Use in Large Language Models</title>
    <link href="https://stephen-smj.tech/2024/04/24/Function%20Calling%20c635408fcd35420d91ac76b97400d9a6/"/>
    <id>https://stephen-smj.tech/2024/04/24/Function%20Calling%20c635408fcd35420d91ac76b97400d9a6/</id>
    <published>2024-04-24T10:02:21.000Z</published>
    <updated>2024-07-26T09:40:18.158Z</updated>
    
    <content type="html"><![CDATA[<p>This article will tell you how to achieve a function calling ability in Large Language Models (LLM) and how to use the function in the LLM.</p><p>Implementation architecture: Client + Service</p><h2 id="Client"><a href="#Client" class="headerlink" title="Client"></a>Client</h2><p>User Input: Query + Function Description (system)<br>Example:<br>Query: Show 5 rows of data.<br>Function Description:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[&#123;</span><br><span class="line">   &quot;name&quot;: &quot;show_data_head&quot;,</span><br><span class="line">   &quot;description&quot;: &quot;Show top n row of data.&quot;,</span><br><span class="line">   &quot;parameters&quot;: &#123;</span><br><span class="line">       &quot;type&quot;: &quot;object&quot;,</span><br><span class="line">       &quot;properties&quot;: &#123;</span><br><span class="line">       &quot;row&quot;: &#123;</span><br><span class="line">           &quot;type&quot;: &quot;string&quot;,</span><br><span class="line">           &quot;description&quot;: &quot;number of rows to show.&quot;</span><br><span class="line">          &#125;</span><br><span class="line">       &#125;</span><br><span class="line">      &#125;</span><br><span class="line">   &#125;,</span><br><span class="line">....</span><br><span class="line">]</span><br></pre></td></tr></table></figure><h2 id="Service"><a href="#Service" class="headerlink" title="Service"></a>Service</h2><p>Use React Template:</p><figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;Answer the following questions as best you can. You have access to the following APIs:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#123;tools_text&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Use the following format:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Question: the input question you must answer</span></span><br><span class="line"><span class="string">Thought: you should always think about what to do</span></span><br><span class="line"><span class="string">Action: the action to take, should be one of [&#123;tools_name_text&#125;]</span></span><br><span class="line"><span class="string">Action Input: the input to the action, if no parameters are provided, marking this as empty.</span></span><br><span class="line"><span class="string">Observation: the result of the action</span></span><br><span class="line"><span class="string">... (this Thought/Action/Action Input/Observation can be repeated zero or more times)</span></span><br><span class="line"><span class="string">Thought: I now know the final answer</span></span><br><span class="line"><span class="string">Final Answer: the final answer to the original input question</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Begin!&quot;</span><span class="string">&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>The {tools_text} is the processed nature langue description of above API description. </p><p>There is an example for processing it to nature langue:</p><figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable constant_">TOOL_DESC_WITH_PARAMETERS</span> = (</span><br><span class="line">    <span class="string">&#x27;&#123;name_for_model&#125;: Call this tool to interact with the &#123;name_for_human&#125; API.&#x27;</span></span><br><span class="line">    <span class="string">&#x27; What is the &#123;name_for_human&#125; API useful for? &#123;description_for_model&#125; Parameters: &#123;parameters&#125;&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p> So, for request above from client. The overall prompt shoud be:</p><figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;Answer the following questions as best you can. You have access to the following APIs:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[</span></span><br><span class="line"><span class="string">&#x27;show_data_head: Call this tool to interact with the show_data_head API. &#x27;</span></span><br><span class="line"><span class="string">    &#x27;What is the show_data_head API useful for? show top n row of data. &#x27;</span></span><br><span class="line"><span class="string">    &#x27;Parameters: &#123;&quot;</span>type<span class="string">&quot;: &quot;</span>object<span class="string">&quot;, &quot;</span>properties<span class="string">&quot;: &#123;&quot;</span>row<span class="string">&quot;: &#123;&quot;</span>type<span class="string">&quot;: &quot;</span>string<span class="string">&quot;, &#x27;</span></span><br><span class="line"><span class="string">    &#x27;&quot;</span>description<span class="string">&quot;: &quot;</span>number <span class="keyword">of</span> rows to show.<span class="string">&quot;&#125;&#125;&#125;\n</span></span><br><span class="line"><span class="string">    ....</span></span><br><span class="line"><span class="string">]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Use the following format:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Question: the input question you must answer</span></span><br><span class="line"><span class="string">Thought: you should always think about what to do</span></span><br><span class="line"><span class="string">Action: the action to take, should be one of [***show_data_head***, ....]</span></span><br><span class="line"><span class="string">Action Input: the input to the action, if no parameters are provided, marking this as empty.</span></span><br><span class="line"><span class="string">Observation: the result of the action</span></span><br><span class="line"><span class="string">... (this Thought/Action/Action Input/Observation can be repeated zero or more times)</span></span><br><span class="line"><span class="string">Thought: I now know the final answer</span></span><br><span class="line"><span class="string">Final Answer: the final answer to the original input question</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Begin!&quot;</span><span class="string">&quot;&quot;</span></span><br></pre></td></tr></table></figure><h3 id="Stopword"><a href="#Stopword" class="headerlink" title="Stopword"></a>Stopword</h3><p>If we let the LLM do text generation by the above react prompt directly, the model will predict the text of <em><strong>Observation: the result of the action</strong></em> directly. That isn’t we want. We want the text here is the result by our API. So it is essentially for make model stop generation in Observation.</p><p>We will add additional stop word to model for let it pauses in the stop words.</p><p>Here, we add “Observation” to the stop text.</p><h3 id="Response"><a href="#Response" class="headerlink" title="Response"></a>Response</h3><p>We will parse the “Action” above to Json formation to return. We refer the response data of Openai here</p><figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;model&quot;</span>: <span class="string">&quot;Qwen&quot;</span>,</span><br><span class="line">  <span class="string">&quot;object&quot;</span>: <span class="string">&quot;chat.completion&quot;</span>,</span><br><span class="line">  <span class="string">&quot;choices&quot;</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">&quot;index&quot;</span>: <span class="number">0</span>,</span><br><span class="line">      <span class="string">&quot;message&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;role&quot;</span>: <span class="string">&quot;assistant&quot;</span>,</span><br><span class="line">        <span class="string">&quot;content&quot;</span>: <span class="string">&quot;Thought: I need to use the show_data_head API to display the first few rows of the data.&quot;</span>,</span><br><span class="line">        <span class="string">&quot;function_call&quot;</span>: &#123;</span><br><span class="line">          <span class="string">&quot;name&quot;</span>: <span class="string">&quot;show_data_head&quot;</span>,</span><br><span class="line">          <span class="string">&quot;arguments&quot;</span>: &#123;<span class="string">&quot;row&quot;</span>: <span class="string">&quot;5&quot;</span>&#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="string">&quot;finish_reason&quot;</span>: <span class="string">&quot;function_call&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">  ],</span><br><span class="line">  <span class="string">&quot;created&quot;</span>: <span class="number">170228275</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="API-Calling-Client"><a href="#API-Calling-Client" class="headerlink" title="API Calling(Client)"></a>API Calling(Client)</h2><p>We got the response Json of LLM. Now, we can call the API and passing the parameters according to Json. Then, we will get the result of our API. </p><figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def <span class="title function_">show_data_head</span>(self, <span class="attr">row</span>: int = <span class="number">5</span>):</span><br><span class="line">    <span class="attr">try</span>:</span><br><span class="line">        #self.<span class="property">message_recorder</span>.<span class="title function_">append</span>(f<span class="string">&quot;&#123;&#125; rows of data: &#123;self.data.head(int(row))&#125;&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> self.<span class="property">data</span>.<span class="title function_">head</span>(<span class="title function_">int</span>(row))</span><br><span class="line">    except <span class="title class_">Exception</span> <span class="keyword">as</span> <span class="attr">e</span>:</span><br><span class="line">        <span class="title function_">print</span>(f<span class="string">&quot;Error in show data: &#123;e&#125;&quot;</span>)</span><br></pre></td></tr></table></figure><p>Result:</p><table><thead><tr><th>Sepal.Length</th><th>Sepal.Width</th><th>Petal.Length</th><th>Petal.Width</th><th>Species</th></tr></thead><tbody><tr><td>5.1</td><td>3.5</td><td>1.4</td><td>0.2</td><td>setosa</td></tr><tr><td>4.9</td><td>3.0</td><td>1.4</td><td>0.2</td><td>setosa</td></tr><tr><td>4.7</td><td>3.2</td><td>1.3</td><td>0.2</td><td>setosa</td></tr><tr><td>4.6</td><td>3.1</td><td>1.5</td><td>0.2</td><td>setosa</td></tr><tr><td>5.0</td><td>3.6</td><td>1.4</td><td>0.2</td><td>setosa</td></tr></tbody></table><p>Importantly, we should add  both of LLM’s response (Json) and  the result of API to message of next request.</p><p>Then, we will call LLM again.</p><h3 id="Second-Response-for-API-Results"><a href="#Second-Response-for-API-Results" class="headerlink" title="Second Response for API Results"></a>Second Response for API Results</h3><p>Note that previously we use template of “chat” for LLM</p><p>But now, we want model do text continuation continuing the “Observation: {result of our API}”. So we should use “completion” template here.</p><figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&lt;completion&gt;</span><br><span class="line">&lt;|im_start|&gt;system</span><br><span class="line"><span class="title class_">You</span> are a data scientist, your mission is help human to <span class="keyword">do</span> data analysis, data mining and generate report.&lt;|im_end|&gt;</span><br><span class="line">&lt;|im_start|&gt;user</span><br><span class="line">hi&lt;|im_end|&gt;</span><br><span class="line">&lt;|im_start|&gt;assistant</span><br><span class="line"><span class="title class_">Hello</span>! <span class="title class_">How</span> can I assist you <span class="keyword">with</span> your data analysis, data mining, or report generation? <span class="title class_">Please</span> provide me <span class="keyword">with</span> some details about the data you have and what insights you<span class="string">&#x27;re hoping to gain, and we can get started.&lt;|im_end|&gt;</span></span><br><span class="line"><span class="string">&lt;|im_start|&gt;user</span></span><br><span class="line"><span class="string">show 5 rows of data&lt;|im_end|&gt;</span></span><br><span class="line"><span class="string">&lt;|im_start|&gt;assistant</span></span><br><span class="line"><span class="string">Thought: I need to use the show_data_head API to display the first few rows of the data.</span></span><br><span class="line"><span class="string">Action: show_data_head</span></span><br><span class="line"><span class="string">Action Input: &#123;&quot;row&quot;: &quot;5&quot;&#125;</span></span><br><span class="line"><span class="string">Observation:    Sepal.Length  Sepal.Width  Petal.Length  Petal.Width Species</span></span><br><span class="line"><span class="string">0           5.1          3.5           1.4          0.2  setosa</span></span><br><span class="line"><span class="string">1           4.9          3.0           1.4          0.2  setosa</span></span><br><span class="line"><span class="string">2           4.7          3.2           1.3          0.2  setosa</span></span><br><span class="line"><span class="string">3           4.6          3.1           1.5          0.2  setosa</span></span><br><span class="line"><span class="string">4           5.0          3.6           1.4          0.2  setosa</span></span><br><span class="line"><span class="string">Thought:</span></span><br><span class="line"><span class="string">&lt;!-- *** --&gt;</span></span><br></pre></td></tr></table></figure><p>The model will do text continuation from the last Thought:</p><figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="title class_">Thought</span>:</span><br><span class="line">&lt;!-- *** --&gt;</span><br><span class="line"><span class="title class_">The</span> <span class="variable constant_">API</span> has displayed the first <span class="number">5</span> rows <span class="keyword">of</span> the data <span class="keyword">as</span> requested.</span><br><span class="line"><span class="title class_">Response</span>: <span class="title class_">Here</span> are the first <span class="number">5</span> rows <span class="keyword">of</span> the <span class="attr">data</span>:</span><br><span class="line"></span><br><span class="line">|<span class="title class_">Sepal</span>.<span class="property">Length</span>| <span class="title class_">Sepal</span>.<span class="property">Width</span>| <span class="title class_">Petal</span>.<span class="property">Length</span>| <span class="title class_">Petal</span>.<span class="property">Width</span>|   <span class="title class_">Species</span>|</span><br><span class="line">|------------|------------|-------------|------------|----------|</span><br><span class="line">|       <span class="number">5.1</span>|         <span class="number">3.5</span>|          <span class="number">1.4</span>|         <span class="number">0.2</span>|    setosa|</span><br><span class="line">|       <span class="number">4.9</span>|         <span class="number">3.0</span>|          <span class="number">1.4</span>|         <span class="number">0.2</span>|    setosa|</span><br><span class="line">|       <span class="number">4.7</span>|         <span class="number">3.2</span>|          <span class="number">1.3</span>|         <span class="number">0.2</span>|    setosa|</span><br><span class="line">|       <span class="number">4.6</span>|         <span class="number">3.1</span>|          <span class="number">1.5</span>|         <span class="number">0.2</span>|    setosa|</span><br><span class="line">|       <span class="number">5.0</span>|         <span class="number">3.6</span>|          <span class="number">1.4</span>|         <span class="number">0.2</span>|    setosa|</span><br><span class="line">&lt;/completion&gt;</span><br></pre></td></tr></table></figure><p>Finally, parse the response again and return to the client.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;This article will tell you how to achieve a function calling ability in Large Language Models (LLM) and how to use the function in the LL</summary>
      
    
    
    
    
    <category term="Deep learning" scheme="https://stephen-smj.tech/tags/Deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>Introduction to Prompt Engineering in Large Language Models (LLMs)</title>
    <link href="https://stephen-smj.tech/2024/03/05/Prompt%20engineering/"/>
    <id>https://stephen-smj.tech/2024/03/05/Prompt%20engineering/</id>
    <published>2024-03-05T05:08:34.000Z</published>
    <updated>2024-09-16T15:38:43.844Z</updated>
    
    <content type="html"><![CDATA[<p>As Large Language Models (LLMs) like GPT-3, T5, and others have become more advanced, <strong>prompt engineering</strong> has emerged as a key skill for controlling and optimizing these models’ outputs. A well-designed prompt can guide the model to generate high-quality, relevant, and coherent results, while a poorly designed one can lead to ambiguous or inaccurate responses.</p><p>In this blog post, we will explore the concept of prompt engineering, how it works, and some best practices to effectively craft prompts for different tasks.</p><h2 id="1-What-is-Prompt-Engineering"><a href="#1-What-is-Prompt-Engineering" class="headerlink" title="1. What is Prompt Engineering?"></a>1. What is Prompt Engineering?</h2><p><strong>Prompt engineering</strong> is the process of designing input instructions (or “prompts”) to elicit desired behaviors or outputs from LLMs. Prompts serve as cues that guide the model in generating responses, and the way the prompt is structured can significantly affect the quality and specificity of the output.</p><p>Prompt engineering often involves:</p><ul><li><strong>Structuring questions or commands</strong>: This guides the model to understand the desired output.</li><li><strong>Providing context</strong>: Adding additional details so the model generates more accurate responses.</li><li><strong>Experimentation</strong>: Iterating and testing different prompts to achieve optimal results.</li></ul><h3 id="Example-of-a-Simple-Prompt"><a href="#Example-of-a-Simple-Prompt" class="headerlink" title="Example of a Simple Prompt:"></a>Example of a Simple Prompt:</h3><ul><li><strong>Prompt</strong>: “Write a short story about a dragon.”</li><li><strong>Output</strong>: The model generates a short story about a dragon based on its training data.</li></ul><h2 id="2-Why-is-Prompt-Engineering-Important"><a href="#2-Why-is-Prompt-Engineering-Important" class="headerlink" title="2. Why is Prompt Engineering Important?"></a>2. Why is Prompt Engineering Important?</h2><p>LLMs have a vast amount of general knowledge, but they are not perfect at handling every task without guidance. A poorly constructed prompt can lead to:</p><ul><li><strong>Vague answers</strong>: The model may generate a generic response.</li><li><strong>Incorrect results</strong>: The model might misinterpret the task.</li><li><strong>Biases or irrelevant information</strong>: The model could include unrelated or biased information.</li></ul><p>By carefully designing prompts, you can:</p><ul><li><strong>Improve output quality</strong>: Get more focused and relevant responses.</li><li><strong>Control creativity</strong>: Guide the model to be more factual or more imaginative, depending on the task.</li><li><strong>Avoid ambiguity</strong>: Reduce the likelihood of the model generating irrelevant or misleading text.</li></ul><h2 id="3-Types-of-Prompts-in-LLMs"><a href="#3-Types-of-Prompts-in-LLMs" class="headerlink" title="3. Types of Prompts in LLMs"></a>3. Types of Prompts in LLMs</h2><p>There are various ways to craft prompts depending on the goal of your task. Here are a few common types of prompts used in LLM interactions:</p><h3 id="3-1-Zero-Shot-Prompting"><a href="#3-1-Zero-Shot-Prompting" class="headerlink" title="3.1. Zero-Shot Prompting"></a>3.1. Zero-Shot Prompting</h3><p>In <strong>zero-shot prompting</strong>, you simply provide the model with a task or question without giving any examples. The model attempts to perform the task based solely on the instruction.</p><p><strong>Example</strong>:</p><ul><li><strong>Prompt</strong>: “Translate the following sentence into French: ‘I love programming.’”</li><li><strong>Output</strong>: “J’aime programmer.”</li></ul><p>Zero-shot prompting is useful when you want a quick response without providing additional context or examples.</p><h3 id="3-2-One-Shot-Prompting"><a href="#3-2-One-Shot-Prompting" class="headerlink" title="3.2. One-Shot Prompting"></a>3.2. One-Shot Prompting</h3><p>In <strong>one-shot prompting</strong>, you provide the model with a single example of the task you want it to perform. This helps the model understand the format or context of the task better than zero-shot prompting.</p><p><strong>Example</strong>:</p><ul><li><p><strong>Prompt</strong>:<br>“Translate the following sentence into French:<br>English: ‘I love programming.’<br>French: ‘J’aime programmer.’<br>Now translate:<br>English: ‘I enjoy learning new languages.’”</p></li><li><p><strong>Output</strong>: “French: ‘J’aime apprendre de nouvelles langues.’”</p></li></ul><h3 id="3-3-Few-Shot-Prompting"><a href="#3-3-Few-Shot-Prompting" class="headerlink" title="3.3. Few-Shot Prompting"></a>3.3. Few-Shot Prompting</h3><p><strong>Few-shot prompting</strong> involves providing the model with multiple examples of the task you want it to perform. This can improve the model’s accuracy and help it understand more complex tasks.</p><p><strong>Example</strong>:</p><ul><li><p><strong>Prompt</strong>:<br>“Translate the following sentences into French:<br>English: ‘I love programming.’<br>French: ‘J’aime programmer.’<br>English: ‘The sun is shining.’<br>French: ‘Le soleil brille.’<br>Now translate:<br>English: ‘I am learning to cook.’”</p></li><li><p><strong>Output</strong>: “French: ‘J’apprends à cuisiner.’”</p></li></ul><p>Few-shot prompting is especially useful when the task is ambiguous or requires the model to understand a pattern.</p><h3 id="3-4-Instruction-Based-Prompting"><a href="#3-4-Instruction-Based-Prompting" class="headerlink" title="3.4. Instruction-Based Prompting"></a>3.4. Instruction-Based Prompting</h3><p>In <strong>instruction-based prompting</strong>, you explicitly describe the task the model needs to perform, often using natural language to provide detailed instructions.</p><p><strong>Example</strong>:</p><ul><li><strong>Prompt</strong>: “Summarize the following paragraph in one sentence: ‘The cat sat on the mat all day long, enjoying the sunshine and occasionally dozing off. It only moved when it heard the rustling of food from the kitchen.’”</li><li><strong>Output</strong>: “The cat spent the day on the mat, enjoying the sun and only moving for food.”</li></ul><h2 id="4-Key-Elements-of-a-Good-Prompt"><a href="#4-Key-Elements-of-a-Good-Prompt" class="headerlink" title="4. Key Elements of a Good Prompt"></a>4. Key Elements of a Good Prompt</h2><p>Designing an effective prompt requires an understanding of several factors that influence the model’s response. Let’s break down some key elements of successful prompt engineering:</p><h3 id="4-1-Clarity-and-Specificity"><a href="#4-1-Clarity-and-Specificity" class="headerlink" title="4.1. Clarity and Specificity"></a>4.1. Clarity and Specificity</h3><p>The more specific and clear your prompt is, the more likely the model will generate a relevant and coherent response. Vague prompts often lead to unpredictable or nonspecific outputs.</p><p><strong>Example of an unclear prompt</strong>:</p><ul><li><strong>Prompt</strong>: “Explain the process.”</li><li><strong>Output</strong>: “Which process are you referring to?”</li></ul><p><strong>Example of a clear prompt</strong>:</p><ul><li><strong>Prompt</strong>: “Explain the process of photosynthesis in plants.”</li><li><strong>Output</strong>: “Photosynthesis is the process by which plants use sunlight to convert carbon dioxide and water into glucose and oxygen.”</li></ul><h3 id="4-2-Contextual-Cues"><a href="#4-2-Contextual-Cues" class="headerlink" title="4.2. Contextual Cues"></a>4.2. Contextual Cues</h3><p>Providing <strong>context</strong> helps the model understand what information to prioritize. Including specific details in the prompt can significantly enhance the quality of the output.</p><p><strong>Example</strong>:</p><ul><li><strong>Prompt</strong>: “In the context of a business presentation, define KPIs.”</li><li><strong>Output</strong>: “In a business presentation, KPIs, or Key Performance Indicators, are measurable values that demonstrate how effectively a company is achieving its objectives.”</li></ul><h3 id="4-3-Instructions-for-Structure"><a href="#4-3-Instructions-for-Structure" class="headerlink" title="4.3. Instructions for Structure"></a>4.3. Instructions for Structure</h3><p>If you want the model to follow a certain structure, provide explicit instructions in the prompt. For example, if you want the output to be in bullet points or numbered lists, state that in the prompt.</p><p><strong>Example</strong>:</p><ul><li><strong>Prompt</strong>: “List three advantages of using renewable energy in bullet points.”</li><li><strong>Output</strong>:  <ul><li>“Reduces greenhouse gas emissions.”  </li><li>“Decreases dependence on fossil fuels.”  </li><li>“Creates sustainable job opportunities.”</li></ul></li></ul><h3 id="4-4-Example-Based-Learning"><a href="#4-4-Example-Based-Learning" class="headerlink" title="4.4. Example-Based Learning"></a>4.4. Example-Based Learning</h3><p>Providing examples or demonstrations helps the model understand how to perform a task better. When the task involves a pattern, showing a few examples can lead to more accurate outputs.</p><p><strong>Example</strong>:</p><ul><li><strong>Prompt</strong>:<br>“Convert the following temperatures from Celsius to Fahrenheit:<br>Celsius: 0<br>Fahrenheit: 32<br>Celsius: 25<br>Fahrenheit: 77<br>Now convert:<br>Celsius: 30”</li><li><strong>Output</strong>: “Fahrenheit: 86.”</li></ul><hr><h2 id="5-Common-Prompt-Engineering-Techniques"><a href="#5-Common-Prompt-Engineering-Techniques" class="headerlink" title="5. Common Prompt Engineering Techniques"></a>5. Common Prompt Engineering Techniques</h2><h3 id="5-1-Chain-of-Thought-Prompting"><a href="#5-1-Chain-of-Thought-Prompting" class="headerlink" title="5.1. Chain-of-Thought Prompting"></a>5.1. Chain-of-Thought Prompting</h3><p><strong>Chain-of-thought prompting</strong> is a technique where you ask the model to generate intermediate reasoning steps before arriving at a final answer. This method is useful for improving the model’s ability to handle complex tasks, like reasoning or math problems.</p><p><strong>Example</strong>:</p><ul><li><strong>Prompt</strong>: “If Sarah has 3 apples and buys 4 more, how many apples does she have? Show your reasoning.”</li><li><strong>Output</strong>: “Sarah starts with 3 apples. She buys 4 more apples, so in total, she has 3 + 4 = 7 apples.”</li></ul><h3 id="5-2-Task-Breakdown"><a href="#5-2-Task-Breakdown" class="headerlink" title="5.2. Task Breakdown"></a>5.2. Task Breakdown</h3><p>When the task is complex, break it down into smaller, more manageable steps. This helps the model focus on each part of the task and generates more accurate results.</p><p><strong>Example</strong>:</p><ul><li><strong>Prompt</strong>:<br>“Step 1: List the ingredients needed to bake a cake.<br>Step 2: Provide a step-by-step process for baking the cake.”</li><li><strong>Output</strong>:  <ul><li>“Step 1: Ingredients: flour, eggs, sugar, butter, milk, baking powder.  </li><li>Step 2: Mix the dry ingredients, add the wet ingredients, bake at 350°F for 30 minutes.”</li></ul></li></ul><hr><h2 id="6-How-to-Optimize-Prompts"><a href="#6-How-to-Optimize-Prompts" class="headerlink" title="6. How to Optimize Prompts"></a>6. How to Optimize Prompts</h2><p>Optimizing prompts often involves <strong>trial and error</strong>. Here are a few strategies to help improve prompt performance:</p><h3 id="6-1-Iterative-Refinement"><a href="#6-1-Iterative-Refinement" class="headerlink" title="6.1. Iterative Refinement"></a>6.1. Iterative Refinement</h3><p>Try multiple variations of your prompt and evaluate the model’s responses. Refine your prompt iteratively to see which version produces the best results.</p><h3 id="6-2-Testing-Edge-Cases"><a href="#6-2-Testing-Edge-Cases" class="headerlink" title="6.2. Testing Edge Cases"></a>6.2. Testing Edge Cases</h3><p>Test your prompt with various inputs, including edge cases or uncommon situations, to ensure the model performs well across different scenarios.</p><h3 id="6-3-Length-Considerations"><a href="#6-3-Length-Considerations" class="headerlink" title="6.3. Length Considerations"></a>6.3. Length Considerations</h3><p>While LLMs can handle long prompts, it’s often more efficient to keep prompts concise. However, if more context or examples are needed for complex tasks, longer prompts may improve output quality.</p><hr><h2 id="7-Conclusion"><a href="#7-Conclusion" class="headerlink" title="7. Conclusion"></a>7. Conclusion</h2><p>Prompt engineering is a crucial aspect of working with Large Language Models. It allows you to control the quality, specificity, and coherence of the model’s outputs. By carefully crafting and optimizing prompts, you can unlock the full potential of LLMs for a wide range of applications, from text generation to task automation.</p><p>Whether you’re using zero-shot, few-shot, or instruction-based prompts, understanding the nuances of prompt engineering will help you guide models to produce better results.</p><hr><h2 id="8-Further-Reading"><a href="#8-Further-Reading" class="headerlink" title="8. Further Reading"></a>8. Further Reading</h2><ul><li><a href="https://beta.openai.com/docs/">OpenAI GPT-3 Documentation</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;As Large Language Models (LLMs) like GPT-3, T5, and others have become more advanced, &lt;strong&gt;prompt engineering&lt;/strong&gt; has emerged as </summary>
      
    
    
    
    
    <category term="Large Language Model" scheme="https://stephen-smj.tech/tags/Large-Language-Model/"/>
    
  </entry>
  
  <entry>
    <title>博客图片被设置防盗链接？我爆肝三晚写了这个批量图片站点转移脚本（已开源）</title>
    <link href="https://stephen-smj.tech/2023/03/21/%E5%8D%9A%E5%AE%A2%E5%9B%BE%E7%89%87%E8%A2%AB%E8%AE%BE%E7%BD%AE%E9%98%B2%E7%9B%97%E9%93%BE%E6%8E%A5%EF%BC%9F%E5%88%AB%E6%85%8C%EF%BC%8C%E6%88%91%E7%88%86%E8%82%9D%E4%B8%89%E6%99%9A%E5%86%99%E4%BA%86%E8%BF%99%E4%B8%AA%E6%89%B9%E9%87%8F%E5%9B%BE%E7%89%87%E7%AB%99%E7%82%B9%E8%BD%AC%E7%A7%BB%E8%84%9A%E6%9C%AC%EF%BC%88%E5%B7%B2%E5%BC%80%E6%BA%90%EF%BC%89/"/>
    <id>https://stephen-smj.tech/2023/03/21/%E5%8D%9A%E5%AE%A2%E5%9B%BE%E7%89%87%E8%A2%AB%E8%AE%BE%E7%BD%AE%E9%98%B2%E7%9B%97%E9%93%BE%E6%8E%A5%EF%BC%9F%E5%88%AB%E6%85%8C%EF%BC%8C%E6%88%91%E7%88%86%E8%82%9D%E4%B8%89%E6%99%9A%E5%86%99%E4%BA%86%E8%BF%99%E4%B8%AA%E6%89%B9%E9%87%8F%E5%9B%BE%E7%89%87%E7%AB%99%E7%82%B9%E8%BD%AC%E7%A7%BB%E8%84%9A%E6%9C%AC%EF%BC%88%E5%B7%B2%E5%BC%80%E6%BA%90%EF%BC%89/</id>
    <published>2023-03-21T01:28:01.000Z</published>
    <updated>2024-07-05T08:09:04.000Z</updated>
    
    <content type="html"><![CDATA[<p>我个人经常在CSDN上进行写作，前一段时间我选择自己建站，为了省事，我直接使用了CSDN上的图片地址，当你在CSDN上写作时你上传的图片会存储在CSDN的服务器中，可以直接在网页上输入URL访问。但是上周CSDN开始对图片施加防盗链接了，我个人网站上的所有图片全都看不到了，并附上一句话：源站可能有防盗链机制,建议将图片保存下来直接上传。</p><p><img src="https://user-images.githubusercontent.com/67999981/226625933-810ee45f-d662-4b7c-a5c5-55519dae2a83.png" alt="image"></p><p>我最近也把博客地址放到简历里了，一想到hr打开我的网站看到一堆报错，肯定会大打折扣，直接pass在简历关TAT。搜索了一些解决方案，大多都治标不治本。目前图片还能存在哪里呢，大部分网站都有防盗链接，虽然github还没有，但是未来某一天也可能会设置防盗链接，毕竟谁愿意给你一直给你白嫖计算资源啊。于是乎我决定把图片存自己云服务器上。然而  一张一张下载 ——&gt; 上传到服务器 —–&gt; 挨个修改md里的图片地址，这样的流程如果手动去做恐怕要搞好几天吧。于是我立马爆肝三个晚上写一个批量转移的脚本并开源，一劳永逸！没有现成的轮子咱自己造轮子！<br><br>项目github地址：<a href="https://github.com/Stephen-SMJ/BlogImageTransfer">https://github.com/Stephen-SMJ/BlogImageTransfer</a><br>这个脚本适合与和我情况类似的朋友们使用: <br><br>1.个站图片被防盗链接限制 2.有云服务器</p><h2 id="脚本使用方法"><a href="#脚本使用方法" class="headerlink" title="脚本使用方法"></a>脚本使用方法</h2><ol><li>克隆仓库并进入项目目录<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/Stephen-SMJ/BlogImageTransfer.git</span><br><span class="line">cd blog_img_script</span><br></pre></td></tr></table></figure></li><li>执行图片下载脚本（folder_path为你本地的md文件存储路径，如果本地没存要先去网站上下载到本地） <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python collection.py --folder_path &quot;the output path of your download images&quot; --output_path ./imgs/</span><br></pre></td></tr></table></figure><img src="https://user-images.githubusercontent.com/67999981/226603242-56024361-6c90-4b1d-81c3-eb4acb433f57.png" alt="屏幕截图 2023-03-21 163156"></li><li>此时你博客文件夹中的所有图片都被下载到了–output_path 这个目录，你需要把这个文件夹上传至你的服务器，并配置Nginx映射</li><li>修改所有md文件中的图片路径(只更域名，会自动保留原图片名称以及描述)<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python change_address.py --folder_path &quot;the address of your md file in your blog project &quot; --server_address &quot;nginx中配置好的图片url&quot;</span><br></pre></td></tr></table></figure> <img src="https://user-images.githubusercontent.com/67999981/226602754-4a2a94c4-e018-4029-84ea-1e9b013df77a.png" alt="屏幕截图 2023-03-21 191719"></li></ol><p>完成，此时再访问你自己的博客网站应该就可以正常显示图片了！<br><br>项目github地址：<a href="https://github.com/Stephen-SMJ/BlogImageTransfer">https://github.com/Stephen-SMJ/BlogImageTransfer</a><br><strong>如果觉得好用麻烦高抬贵手点点star！</strong></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;我个人经常在CSDN上进行写作，前一段时间我选择自己建站，为了省事，我直接使用了CSDN上的图片地址，当你在CSDN上写作时你上传的图片会存储在CSDN的服务器中，可以直接在网页上输入URL访问。但是上周CSDN开始对图片施加防盗链接了，我个人网站上的所有图片全都看不到了，</summary>
      
    
    
    
    
    <category term="Python" scheme="https://stephen-smj.tech/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Nginx反向代理中的坑</title>
    <link href="https://stephen-smj.tech/2023/03/03/Nginx%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%E4%B8%AD%E7%9A%84%E6%B7%B1%E5%9D%91/"/>
    <id>https://stephen-smj.tech/2023/03/03/Nginx%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%E4%B8%AD%E7%9A%84%E6%B7%B1%E5%9D%91/</id>
    <published>2023-03-03T10:02:21.000Z</published>
    <updated>2024-07-05T08:09:04.000Z</updated>
    
    <content type="html"><![CDATA[<p>今天完成了一个人脸识别系统的开发，在开发环境中调试好之后打算部署到服务器上。由于系统需要调用用户摄像头需要使用https协议访问。而我自己的域名没有在公安局备份，没办法申请SSL证书。于是我借了朋友的一个域名，用Nginx反向代理一下。但是我在生产环境中部署好后端项目之后发现系统无法通过登录验证，同样的代码在开发环境中完全没有问题，经过排查之后发现是用户Token丢失。于是乎想到了肯定是Nginx的锅，搜索了一些资料和文档后，才知道Nginx进行代理时，有一个默认的参数：<strong>underscores_in_headers</strong>，<br>这个参数默认为false。这个参数的作用是是否转发<strong>带下划线的header</strong>，而很多人并不知道header的命名规范是最好不要带下划线的。因此如果不把参数设置为True的话，在转发之后header会被去掉。</p><p>重新配置Nginx：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">  ...</span><br><span class="line">  underscores_in_headers on;</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>完美解决！</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;今天完成了一个人脸识别系统的开发，在开发环境中调试好之后打算部署到服务器上。由于系统需要调用用户摄像头需要使用https协议访问。而我自己的域名没有在公安局备份，没办法申请SSL证书。于是我借了朋友的一个域名，用Nginx反向代理一下。但是我在生产环境中部署好后端项目之后发</summary>
      
    
    
    
    
    <category term="Back-end" scheme="https://stephen-smj.tech/tags/Back-end/"/>
    
  </entry>
  
  <entry>
    <title>Git企业级分支提交流程</title>
    <link href="https://stephen-smj.tech/2023/03/01/Git%20%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%88%86%E6%94%AF%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B/"/>
    <id>https://stephen-smj.tech/2023/03/01/Git%20%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%88%86%E6%94%AF%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B/</id>
    <published>2023-03-01T10:02:21.000Z</published>
    <updated>2024-07-07T08:56:52.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://gist.github.com/assets/67999981/2d402a54-45a9-44c5-85fb-a026581ccad9" alt="d42dcb7d494a45cdb08754bce73127e3"></p><ol><li><p>首先在本地分支hfdev上进行开发，开发后要经过测试。</p></li><li><p>如果测试通过了，那么久可以合并到本地分支develop，合并之后hfdev和development应该完全一样。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">git add 文件</span><br><span class="line">git commit -m ‘注释’</span><br><span class="line">git checkout develop //切换到develop分支</span><br><span class="line">git merge --no-ff -m &#x27;合并&#x27; lhfdev </span><br><span class="line">git diff develop lhfdev</span><br></pre></td></tr></table></figure></li><li><p>接下来将本地分支develop提交到远程分支develop，注意提交之前要先pull一下，因为团队中还有其他人在提交代码，不pull会产生版本冲突</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git pull origin develop</span><br><span class="line">git push origin develop</span><br></pre></td></tr></table></figure></li><li><p>接下来将develop分支的代码合并到main分支</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git checkout main</span><br><span class="line">git merge --no-ff -m &#x27;合并到main&#x27; develop</span><br><span class="line">git diff develop main //查看是否有不同</span><br></pre></td></tr></table></figure></li><li><p>将本地mian分支提交到远程</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git pull origin main</span><br><span class="line">git push origin main</span><br></pre></td></tr></table></figure></li></ol><p>以上就完成了一次基本的提交操作。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://gist.github.com/assets/67999981/2d402a54-45a9-44c5-85fb-a026581ccad9&quot; alt=&quot;d42dcb7d494a45cdb08754bce73127e3&quot;&gt;&lt;/p&gt;
&lt;ol&gt;
</summary>
      
    
    
    
    
    <category term="Git" scheme="https://stephen-smj.tech/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>剑指offer：在排序数组中查找数字I的个数</title>
    <link href="https://stephen-smj.tech/2023/03/01/%E5%9C%A8%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E6%9F%A5%E6%89%BE%E6%95%B0%E5%AD%97I%E7%9A%84%E4%B8%AA%E6%95%B0/"/>
    <id>https://stephen-smj.tech/2023/03/01/%E5%9C%A8%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E6%9F%A5%E6%89%BE%E6%95%B0%E5%AD%97I%E7%9A%84%E4%B8%AA%E6%95%B0/</id>
    <published>2023-03-01T10:02:21.000Z</published>
    <updated>2024-07-07T08:54:16.000Z</updated>
    
    <content type="html"><![CDATA[<p>LeetCode 2698题，剑指offer 53题。<br>击败100%的用户<br><img src="https://gist.github.com/assets/67999981/c43c0214-92f9-4fbb-b383-01b283fa021b" alt="a8ed2264ccb8471dbd3959bd9d3b3703"></p><h3 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h3><p>统计一个数字在排序数组中出现的次数。</p><p>示例 1:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">输入: nums = [5,7,7,8,8,10], target = 8</span><br><span class="line">输出: 2</span><br></pre></td></tr></table></figure><p>示例 2:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">输入: nums = [5,7,7,8,8,10], target = 6</span><br><span class="line">输出: 0</span><br></pre></td></tr></table></figure><p>看到这个题目的第一反应是，如果面试遇到这个题我肯定会笑出来，直接循环计数就好了。但是仔细一想，这样的题如果只能给出一个O(N)的解法，恐怕会被当场终止面试。利用题目中从小到大排序的特性，首先想到了二分查找，时间复杂度能优化到O(log2n)，但是普通的二分查找返回的target的位置是不确定的（当有多个重复target时），因此我们需要对二分查找做修改：</p><p><em><strong>算法思路</strong></em>：</p><p>1.找到数组中最左边的target的位置idl ： 将普通的二分查找改为修改找到最左边（数组中第一个）target的位置</p><p>2.找到数组中最右边的target的位置idr ： 将普通的二分查找改为修改找到最右边（数组中最后一个）target的位置</p><p>3.idr-idl+1即为数组中target的个数</p><p><em><strong>修改细节</strong></em>：</p><p>普通二分查找当target等于nums[mid]时返回mid，但由于我们找到第一个之后还要找第二个。</p><p>因此如果是往左边找的话：</p><ul><li><em>在target = nums[mid]的情况下要 right = mid - 1;</em> </li><li><em>此时循环结束后的left就是第一个target的下标。//因为当mid小于target时一直在移动left，等于target时才开始移动right。</em></li></ul><p>​<br>那么如果是往右找的话：</p><ul><li><em>在target = nums[mid]的情况下要 left = mid + 1;</em></li><li><em>此时循环结束后的right就是最后一个target的下标。</em></li></ul><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">class Solution &#123;</span><br><span class="line">    public int search(int[] nums, int target) &#123;</span><br><span class="line">        int lefId = leftBinarySearch(nums,target);</span><br><span class="line">        int rightId = rightBinarySearch(nums, target);</span><br><span class="line">        if (lefId == -1)&#123;</span><br><span class="line">            return 0;</span><br><span class="line">        &#125;</span><br><span class="line">        else &#123;</span><br><span class="line">            return rightId - lefId + 1;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    public static int leftBinarySearch(int[] nums, int target)&#123;</span><br><span class="line">        int left = 0;</span><br><span class="line">        int right = nums.length-1;</span><br><span class="line">        int mid = (left +  right) / 2;</span><br><span class="line">        while (left &lt;= right)&#123;</span><br><span class="line">            if (target &lt;= nums[mid])&#123;</span><br><span class="line">                right = mid - 1;</span><br><span class="line">            &#125;</span><br><span class="line">            else &#123;</span><br><span class="line">                left = mid + 1;</span><br><span class="line">            &#125;</span><br><span class="line">            mid = (left + right) / 2;</span><br><span class="line">        &#125;</span><br><span class="line">        if (left &lt; nums.length &amp;&amp; nums[left] == target)&#123;</span><br><span class="line">            return left;</span><br><span class="line">        &#125;</span><br><span class="line">        return -1;</span><br><span class="line">    &#125;</span><br><span class="line">    public static int rightBinarySearch(int[] nums, int target)&#123;</span><br><span class="line">        int left = 0;</span><br><span class="line">        int right = nums.length-1;</span><br><span class="line">        int mid = (left + right) / 2;</span><br><span class="line">        while (left &lt;= right)&#123;</span><br><span class="line">            if (target &gt;= nums[mid])&#123;</span><br><span class="line">                left = mid + 1;</span><br><span class="line">            &#125;</span><br><span class="line">            else if (target &lt; nums[mid])&#123;</span><br><span class="line">                right = mid - 1;</span><br><span class="line">            &#125;</span><br><span class="line">            mid = (left + right) / 2;</span><br><span class="line">        &#125;</span><br><span class="line">        if (right &gt;= 0 &amp;&amp; nums[right]==target)&#123;</span><br><span class="line">            return right;</span><br><span class="line">        &#125;</span><br><span class="line">        return -1;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;LeetCode 2698题，剑指offer 53题。&lt;br&gt;击败100%的用户&lt;br&gt;&lt;img src=&quot;https://gist.github.com/assets/67999981/c43c0214-92f9-4fbb-b383-01b283fa021b&quot; alt=&quot;</summary>
      
    
    
    
    
    <category term="Algorithms" scheme="https://stephen-smj.tech/tags/Algorithms/"/>
    
  </entry>
  
  <entry>
    <title>Calculate Forward and Back Propagation in Neural Networks</title>
    <link href="https://stephen-smj.tech/2023/02/28/Forward%20Propagation%20and%20Back%20Propagation/"/>
    <id>https://stephen-smj.tech/2023/02/28/Forward%20Propagation%20and%20Back%20Propagation/</id>
    <published>2023-02-28T10:02:21.000Z</published>
    <updated>2024-09-16T15:33:02.204Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Forward-Propagation-and-Back-Propagation"><a href="#Forward-Propagation-and-Back-Propagation" class="headerlink" title="Forward Propagation and Back Propagation"></a>Forward Propagation and Back Propagation</h1><p><img src="https://s2.loli.net/2024/09/16/vrDfM3iu5JoCb87.jpg" alt="221137503-b25175ea-5442-4179-a2da-4d89a71876ba.jpg"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Forward-Propagation-and-Back-Propagation&quot;&gt;&lt;a href=&quot;#Forward-Propagation-and-Back-Propagation&quot; class=&quot;headerlink&quot; title=&quot;Forward Prop</summary>
      
    
    
    
    
    <category term="Deep learning" scheme="https://stephen-smj.tech/tags/Deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>Npm run serve &amp; build 错误-Error-error:0308010C:digital envelope routines unsupported</title>
    <link href="https://stephen-smj.tech/2023/02/20/Npm%20run%20servebuild%20%E9%94%99%E8%AF%AF%EF%BC%9AError%20error0308010Cdigital%20envelope%20routinesunsupported/"/>
    <id>https://stephen-smj.tech/2023/02/20/Npm%20run%20servebuild%20%E9%94%99%E8%AF%AF%EF%BC%9AError%20error0308010Cdigital%20envelope%20routinesunsupported/</id>
    <published>2023-02-20T10:02:21.000Z</published>
    <updated>2024-07-05T08:09:04.000Z</updated>
    
    <content type="html"><![CDATA[<p>出现这个错误的原因</p><p>Baidu 了一下发现是 Node JS 17 的 BUG，相关 ISSUE 也给出了解决办法，就是修改package.json，在相关构建命令之前加入:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set NODE_OPTIONS=–openssl-legacy-provider</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&quot;serve&quot;: &quot;set NODE_OPTIONS=--openssl-legacy-provider &amp; vue-cli-service serve&quot;,</span><br><span class="line">&quot;build&quot;: &quot;set NODE_OPTIONS=--openssl-legacy-provider &amp; vue-cli-service build&quot;,</span><br><span class="line">&quot;build:report&quot;: &quot;set NODE_OPTIONS=--openssl-legacy-provider &amp; vue-cli-service build --report&quot;</span><br></pre></td></tr></table></figure><p>然后再次运行 npm run serve，npm run build就不会报错啦</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;出现这个错误的原因&lt;/p&gt;
&lt;p&gt;Baidu 了一下发现是 Node JS 17 的 BUG，相关 ISSUE 也给出了解决办法，就是修改package.json，在相关构建命令之前加入:&lt;/p&gt;
&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;t</summary>
      
    
    
    
    
    <category term="Front-end" scheme="https://stephen-smj.tech/tags/Front-end/"/>
    
  </entry>
  
  <entry>
    <title>使用Git将项目上传到Github仓库流程</title>
    <link href="https://stephen-smj.tech/2023/02/15/%E4%BD%BF%E7%94%A8Git%E5%B0%86%E6%96%B0%E9%A1%B9%E7%9B%AE%E4%B8%8A%E4%BC%A0%E5%88%B0github/"/>
    <id>https://stephen-smj.tech/2023/02/15/%E4%BD%BF%E7%94%A8Git%E5%B0%86%E6%96%B0%E9%A1%B9%E7%9B%AE%E4%B8%8A%E4%BC%A0%E5%88%B0github/</id>
    <published>2023-02-15T10:02:21.000Z</published>
    <updated>2024-07-05T08:09:04.000Z</updated>
    
    <content type="html"><![CDATA[<ol><li>首先在你本地的项目文件夹初始化git，最好指定名称为main，因为新版本的git远程仓库默认分支为main，大家估计都还习惯用matser，所以在提交的时候很容易创建出来两个分支，因此在这里初始化的时候就直接指定分支名称为main，以后就不用改了。</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git init -b main //初始化，生成.git文件夹，本地分支名称为main</span><br></pre></td></tr></table></figure><ol start="2"><li>新建一个远程仓库，名称为origin，并与本地分支联系起来（一定要分清仓库和分支的概念）</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git remote add origin git@github.com //git添加远程仓库</span><br></pre></td></tr></table></figure><ol start="3"><li>将本地项目文件添加到本地仓库 （.代表所有文件，单独文件的话可以用文件名）</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git add .</span><br></pre></td></tr></table></figure><ol start="4"><li>将本地仓库中的文件提交到远程仓库去，并附带一条message，告诉其他人你为什么要上传这个文件，修改了哪些东西</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git commit -m &#x27;upload&#x27;</span><br></pre></td></tr></table></figure><ol start="5"><li>将远程仓库中的文件推送到main分支去，这样就能在github仓库中看到全部的文件了。</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push origin main:main</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;ol&gt;
&lt;li&gt;首先在你本地的项目文件夹初始化git，最好指定名称为main，因为新版本的git远程仓库默认分支为main，大家估计都还习惯用matser，所以在提交的时候很容易创建出来两个分支，因此在这里初始化的时候就直接指定分支名称为main，以后就不用改了。&lt;/li&gt;
&lt;</summary>
      
    
    
    
    
    <category term="Git" scheme="https://stephen-smj.tech/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>如何将你的前端项目上传到Github仓库并且部署到github pages</title>
    <link href="https://stephen-smj.tech/2023/02/10/%E5%A6%82%E4%BD%95%E5%B0%86%E4%BD%A0%E7%9A%84%E5%89%8D%E7%AB%AF%E9%A1%B9%E7%9B%AE%E4%B8%8A%E4%BC%A0%E5%88%B0Github%E4%BB%93%E5%BA%93%E5%B9%B6%E4%B8%94%E9%83%A8%E7%BD%B2/"/>
    <id>https://stephen-smj.tech/2023/02/10/%E5%A6%82%E4%BD%95%E5%B0%86%E4%BD%A0%E7%9A%84%E5%89%8D%E7%AB%AF%E9%A1%B9%E7%9B%AE%E4%B8%8A%E4%BC%A0%E5%88%B0Github%E4%BB%93%E5%BA%93%E5%B9%B6%E4%B8%94%E9%83%A8%E7%BD%B2/</id>
    <published>2023-02-10T10:02:21.000Z</published>
    <updated>2024-07-07T08:43:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="新建仓库"><a href="#新建仓库" class="headerlink" title="新建仓库"></a>新建仓库</h2><p>仓库名称为你的项目名，可以生成一个readme文件，不生成也行。<br><img src="https://gist.github.com/assets/67999981/3006f1c6-c08a-43b7-a09b-0b57c179212b" alt="e44216169e9f4cac92175521cb591c95"></p><h2 id="项目打包"><a href="#项目打包" class="headerlink" title="项目打包"></a>项目打包</h2><p>首先打开你的.gitignore文件看看有没有/dist，如果有的话要删掉这一行，因为这个文件的作用就是告诉git哪些文件不上传，而我们要上传的就是dist，所以必须删除掉<br><img src="https://gist.github.com/assets/67999981/3c317b2b-810b-40fd-8323-2996508b84e3" alt="368f86a262b84071aee3de72d7bb177b"></p><p>在本地根目录下运行</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm run build</span><br></pre></td></tr></table></figure><p>然后你就会发现文件夹中生成了一个dist文件夹，里面是打包好的原生的html, css, js文件，此时你打开index.html应该能直接在网页中看到样式，如果看不到那就说明打包有错误，F12查看报错并解决。</p><p><img src="https://gist.github.com/assets/67999981/751e1f09-36e9-43eb-a561-3b8f254d63aa" alt="136ad5c2c6814a90ade1245356ea95af"></p><h2 id="上传项目"><a href="#上传项目" class="headerlink" title="上传项目"></a>上传项目</h2><p>将你本地的项目上传到github仓库中，具体步骤可以看我之前写的这篇博客：<a href="https://blog.csdn.net/qq_44785351/article/details/129320983?spm=1001.2014.3001.5501">使用Git上传项目到Github仓库中</a></p><h2 id="打包并发布"><a href="#打包并发布" class="headerlink" title="打包并发布"></a>打包并发布</h2><p>接下来点击settings -&gt; pages， 在source中选择deploy from a branch, branch 选择你要部署的branch，目录选择root，点击save。</p><p><img src="https://gist.github.com/assets/67999981/731fec3e-4ac5-4ab9-af8e-8293bcddd6b2" alt="a2e5dc5484d943c18ecd7955e13b4d65"></p><p>可以在actions中查看构建是否完毕，没报错的话直接访问地址: 仓库地址+/dist/ 就能看到了，毕竟打包的项目 <code>index.html</code> 在这个目录下</p><p><img src="https://gist.github.com/assets/67999981/f10af9de-e8a3-4832-b971-ab6ebce88676" alt="1f077aa08433496b97bfad732fd3cc3f"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;新建仓库&quot;&gt;&lt;a href=&quot;#新建仓库&quot; class=&quot;headerlink&quot; title=&quot;新建仓库&quot;&gt;&lt;/a&gt;新建仓库&lt;/h2&gt;&lt;p&gt;仓库名称为你的项目名，可以生成一个readme文件，不生成也行。&lt;br&gt;&lt;img src=&quot;https://gist.git</summary>
      
    
    
    
    
    <category term="Git" scheme="https://stephen-smj.tech/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>一道很考验数据结构与算法的功底的笔试题：用JAVA设计一个缓存结构</title>
    <link href="https://stephen-smj.tech/2023/02/10/%E7%94%A8JAVA%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E7%BC%93%E5%AD%98%E7%BB%93%E6%9E%84/"/>
    <id>https://stephen-smj.tech/2023/02/10/%E7%94%A8JAVA%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E7%BC%93%E5%AD%98%E7%BB%93%E6%9E%84/</id>
    <published>2023-02-10T08:40:34.000Z</published>
    <updated>2024-07-05T08:09:04.000Z</updated>
    
    <content type="html"><![CDATA[<p>我在上周的笔试中遇到了这样一道题目，觉得有难度而且很考验数据结构与算法的功底，因此Mark一下。</p><h2 id="需求说明"><a href="#需求说明" class="headerlink" title="需求说明"></a>需求说明</h2><p>设计并实现一个缓存数据结构:<br><br>该数据结构具有以下功能：<br><br>get(key)<br> 如果指定的key存在于缓存中，则返回与该键关联的值，则返回-1。<br><br>put(key、val、weight)<br><br>将值与缓存中的键关联，以便以后可以通过get(key)检索值。<br><br>缓存具有固定的容量，当达到该容量时，score最小的密钥必须失效，直到密钥的数量落在缓存容量之内。<br>score的计算方法如下：<br><br>weight ∕ [ln(current_time - last_accessed_time + 1) + 1]<br><br>缓存的实现需要get(key)的时间复杂度为O(1)。为了实现高速缓存，您可以假设可用一些常见的数据结构，如数组、不同类型的列表和哈希表。在答案的最后，给出并解释get(key)和放入put(key)的计算复杂度</p><h3 id="我的思路"><a href="#我的思路" class="headerlink" title="我的思路"></a>我的思路</h3><p>首先，一说到get和put，肯定会想到哈希map，并且哈希的get时间复杂度也为O(1),符合要求，但比较棘手的需求是如何实现缓存的score机制，当缓存满的时候需要让score最低的节点drop掉。苦思冥想之后我想到了优先队列(priority queue)，平时觉得这个数据结构很冷门，但确实有应用场景，优先队列是一种根据权重进行出队顺序排列的队列，那么我只需要将题目中的score定位为权重就行了。<br>此时我又想到了用JAVA中的Comparator去定义一个这样的权重策略，因为优先队列的权重是可以被Comparator重写的。所以我总共需要用到两个数据结构。<br>用hashmap实现get和put的一一对应，同时将节点存入优先队列，当容量满时让score小的出队就行了。（注意，Java中优先队列是权重小的先出队）</p><h2 id="我的答案"><a href="#我的答案" class="headerlink" title="我的答案"></a>我的答案</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line">import java.util.*;</span><br><span class="line"></span><br><span class="line">class Node&#123;</span><br><span class="line">int key;</span><br><span class="line">int val;</span><br><span class="line">int weight;</span><br><span class="line">int timeStamp;</span><br><span class="line"></span><br><span class="line">public Node(int key, int val, int weight, int timeStamp) &#123;</span><br><span class="line">this.key = key;</span><br><span class="line">this.val = val;</span><br><span class="line">this.weight = weight;</span><br><span class="line">this.timeStamp = timeStamp;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public class Cache &#123;</span><br><span class="line">int capacity;</span><br><span class="line">int timeStamp;</span><br><span class="line">Map&lt;Integer,Node&gt; nodeMap;  //k-v mapping</span><br><span class="line">PriorityQueue&lt;Node&gt; prque;  //store the node</span><br><span class="line"></span><br><span class="line">public Cache(int capacity)&#123;</span><br><span class="line">this.capacity = capacity;</span><br><span class="line">this.timeStamp = 0;</span><br><span class="line">nodeMap = new HashMap&lt;&gt;();</span><br><span class="line">Comparator&lt;Node&gt; timeWeightComparator = new Comparator&lt;Node&gt;() &#123; //rewrite the priority</span><br><span class="line">@Override</span><br><span class="line">public int compare(Node o1, Node o2) &#123;</span><br><span class="line">return (int) (o1.weight / (Math.log(o1.timeStamp - o2.timeStamp + 1) + 1) -</span><br><span class="line">(o2.weight / (Math.log(o2.timeStamp - o1.timeStamp + 1) + 1)));</span><br><span class="line">&#125;</span><br><span class="line">&#125;;</span><br><span class="line">prque = new PriorityQueue&lt;&gt;(timeWeightComparator);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public int get(int key)&#123;  //时间复杂度O(1), hashmap.get为O(1)</span><br><span class="line">if (!nodeMap.containsKey(key))&#123;</span><br><span class="line">return -1;</span><br><span class="line">&#125;</span><br><span class="line">Node getNode = nodeMap.get(key);</span><br><span class="line">getNode.timeStamp = ++timeStamp;</span><br><span class="line">return getNode.val;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void put(int key, int val, int weight)&#123; //最好的情况是已经包含这个键了,时间复杂度为O(1)</span><br><span class="line">if (this.capacity &lt;= 0)&#123;</span><br><span class="line">return;</span><br><span class="line">&#125;</span><br><span class="line">if (nodeMap.containsKey(key))&#123;</span><br><span class="line">Node newNode = nodeMap.get(key);</span><br><span class="line">newNode.val = val;</span><br><span class="line">newNode.weight = weight;</span><br><span class="line">newNode.timeStamp = ++ timeStamp;</span><br><span class="line">&#125;else &#123;</span><br><span class="line">if (nodeMap.size() == capacity)&#123;</span><br><span class="line">Node leastNode = prque.poll(); //O(logN)</span><br><span class="line">assert leastNode != null;</span><br><span class="line">nodeMap.remove(leastNode.key);</span><br><span class="line">&#125;</span><br><span class="line">Node newNode = new Node(key, val, weight, ++timeStamp);</span><br><span class="line">prque.add(newNode);</span><br><span class="line">nodeMap.put(key,newNode);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123; //test case</span><br><span class="line">Cache cache = new Cache(5);</span><br><span class="line">cache.put(0,15,3);</span><br><span class="line">cache.put(1,28,10);</span><br><span class="line">cache.put(2,16,4);</span><br><span class="line">cache.put(3,4,6);</span><br><span class="line">cache.put(4,75,5);</span><br><span class="line">cache.put(4,100,100);</span><br><span class="line">System.out.println(cache.get(1));</span><br><span class="line">System.out.println(cache.get(2));</span><br><span class="line">System.out.println(cache.get(3));</span><br><span class="line">System.out.println(cache.get(4));</span><br><span class="line">System.out.println(cache.get(0));</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="BigO-notation-analysis"><a href="#BigO-notation-analysis" class="headerlink" title="BigO notation analysis"></a>BigO notation analysis</h1><h2 id="get"><a href="#get" class="headerlink" title="get"></a>get</h2><p>The get operation is base on the hashmap.get(key). So, the time complexity is O(1).</p><h2 id="put"><a href="#put" class="headerlink" title="put"></a>put</h2><h3 id="The-put-operation-can-be-seperated-to-follow-two-case"><a href="#The-put-operation-can-be-seperated-to-follow-two-case" class="headerlink" title="The put operation can be seperated to follow two case:"></a>The put operation can be seperated to follow two case:</h3><h4 id="1-Don’t-need-insert-a-new-node-when-the-key-is-exist"><a href="#1-Don’t-need-insert-a-new-node-when-the-key-is-exist" class="headerlink" title="1. Don’t need insert a new node (when the key is exist)"></a>1. Don’t need insert a new node (when the key is exist)</h4><p>In this case, we only need to get the node from hashmap and update it. The time complexity is O(1).</p><h4 id="2-Insert-new-Node"><a href="#2-Insert-new-Node" class="headerlink" title="2. Insert new Node"></a>2. Insert new Node</h4><p>If the capcity is not reached. we can insert a new node directly. the complexity is O(logN) + O(1) = O(logN) —- (O(logN) for priorityque, O(1) for hashmap).</p><p>If the capicity is reached. we need to poll a node with least score, the time complexity is O(logN). Then inster a new node. The time complexity is O(logN) + O(logN) + O(1) = O(logN).</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;我在上周的笔试中遇到了这样一道题目，觉得有难度而且很考验数据结构与算法的功底，因此Mark一下。&lt;/p&gt;
&lt;h2 id=&quot;需求说明&quot;&gt;&lt;a href=&quot;#需求说明&quot; class=&quot;headerlink&quot; title=&quot;需求说明&quot;&gt;&lt;/a&gt;需求说明&lt;/h2&gt;&lt;p&gt;设计并实现一</summary>
      
    
    
    
    
    <category term="JAVA" scheme="https://stephen-smj.tech/tags/JAVA/"/>
    
  </entry>
  
  <entry>
    <title>解决npm run build 之后生成的index.html页面打开为空</title>
    <link href="https://stephen-smj.tech/2023/02/05/%E8%A7%A3%E5%86%B3npm%20run%20build%20%E4%B9%8B%E5%90%8E%E7%94%9F%E6%88%90%E7%9A%84index.html%E9%A1%B5%E9%9D%A2%E6%89%93%E5%BC%80%E4%B8%BA%E7%A9%BA/"/>
    <id>https://stephen-smj.tech/2023/02/05/%E8%A7%A3%E5%86%B3npm%20run%20build%20%E4%B9%8B%E5%90%8E%E7%94%9F%E6%88%90%E7%9A%84index.html%E9%A1%B5%E9%9D%A2%E6%89%93%E5%BC%80%E4%B8%BA%E7%A9%BA/</id>
    <published>2023-02-05T10:02:21.000Z</published>
    <updated>2024-07-07T08:33:58.000Z</updated>
    
    <content type="html"><![CDATA[<p>今天在用npm构建前端项目之后发现生成dist文件夹中的页面是空的，于是F12看了一下发现有报错：</p><p><img src="https://gist.github.com/assets/67999981/06d5fd9b-f79e-428b-828a-e56c69e3b5be" alt="cb34d61f0f2f4bd88a3f14087a5d262a"></p><p>这个报错很显然是找不到文件，因此我猜可能是构建项目过程中路径出错了。打开index.html的源码来看，发现路径用的是’/‘，但index.html文件和js文件夹是同级目录，如果从index.html进入到js文件夹内的文件，需要用’./‘。<br>也就是说，对比dist文件夹结构可以看到资源路径的引入是错误的，**应该用’./‘而不是’/‘**。</p><p>结合了百度，发现一个有效的解决方案：</p><p>在项目根目录下创建一个vue.config.js文件，写入：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">module.exports = &#123;</span><br><span class="line">    publicPath: &#x27;./&#x27;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后再次npm run build进行打包。打包完成后，再打开dist文件夹内的index.html文件，就可以正常显示项目了。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;今天在用npm构建前端项目之后发现生成dist文件夹中的页面是空的，于是F12看了一下发现有报错：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://gist.github.com/assets/67999981/06d5fd9b-f79e-428b-828a-e56c69</summary>
      
    
    
    
    
    <category term="Front-end" scheme="https://stephen-smj.tech/tags/Front-end/"/>
    
  </entry>
  
  <entry>
    <title>Java循环和Python循环的区别</title>
    <link href="https://stephen-smj.tech/2023/02/01/JAVA%E5%BE%AA%E7%8E%AF%E5%92%8CPython%E5%BE%AA%E7%8E%AF%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
    <id>https://stephen-smj.tech/2023/02/01/JAVA%E5%BE%AA%E7%8E%AF%E5%92%8CPython%E5%BE%AA%E7%8E%AF%E7%9A%84%E5%8C%BA%E5%88%AB/</id>
    <published>2023-02-01T10:27:52.000Z</published>
    <updated>2024-07-07T06:36:40.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>一句话总结</strong>：JAVA循环中的变量i是动态分配的，可以被改变的。而Python循环中的i是在初始化时就被分配在了内存中，无法改变。实验：</p><p>JAVA for循环：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">int count  = 0;</span><br><span class="line">for (int i=0; i&lt;6; i++)&#123;</span><br><span class="line">i++;</span><br><span class="line">System.out.println(i);</span><br><span class="line">count++;</span><br><span class="line">&#125;</span><br><span class="line">System.out.println(count);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>输出结果：<br>1<br>3<br>5<br>count : 3<br>也就是只循环了3次，因为我们在循环中改变了i的值。</p><p>Python for循环：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">count = 0</span><br><span class="line">for i in range(6):</span><br><span class="line">    i += 1</span><br><span class="line">    print(i)</span><br><span class="line">    count += 1</span><br><span class="line">print(&quot;count:&quot; + str(count))</span><br></pre></td></tr></table></figure><p>输出结果：<br>1<br>2<br>3<br>4<br>5<br>6<br>count:6<br>可见，在循环内部修改的i在下次循环时还会变回默认的值，因为range(6)相当于直接分配了一个[0~5]的数组，i会在这个数组中遍历取值。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;一句话总结&lt;/strong&gt;：JAVA循环中的变量i是动态分配的，可以被改变的。而Python循环中的i是在初始化时就被分配在了内存中，无法改变。实验：&lt;/p&gt;
&lt;p&gt;JAVA for循环：&lt;/p&gt;
&lt;figure class=&quot;highlight plai</summary>
      
    
    
    
    
    <category term="JAVA" scheme="https://stephen-smj.tech/tags/JAVA/"/>
    
  </entry>
  
</feed>
